{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simplification_Baselines.ipynb\"\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zAgDRY4Qh88"
      },
      "source": [
        "To evaluate simplification quality 2 baselines will be used:\n",
        "\n",
        "* Simple trunctuation baseline: leave only a certain number of the first words of a sentence\n",
        "* GPT generation (finetuned russian gpt-3 model from Sber)\n",
        "\n",
        "As the test set I will use the dev part of the Russian dataset collected via Toloka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZjjBgC3RHTu"
      },
      "source": [
        "# Trunctuation Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgPg72OqpZQH"
      },
      "source": [
        "##Loading data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFbDvfYM1UHj"
      },
      "source": [
        "! pip install textstat\n",
        "! pip install --upgrade language_tool_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpBsUsh3WiXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae8c254-5853-411d-8d95-1a33006fc351"
      },
      "source": [
        "import nltk\n",
        "import textstat\n",
        "import language_tool_python\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFQxYnghRutn"
      },
      "source": [
        "# test_data2 = pd.read_csv('/content/drive/MyDrive/SImplification_models/hidden_test_sents.csv')\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/SImplification_models/public_test_sents.csv')#, error_bad_lines=False, engine='python')\n",
        "# full_test = pd.concat((test_data, test_data2))\n",
        "# test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaaeMuodY7VH"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/dialogue-evaluation/RuSimpleSentEval/main/dev_sents.csv\n",
        "test_data = pd.read_csv('/content/dev_sents.csv')\n",
        "test_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXUe52_WwU6m"
      },
      "source": [
        "First look at the test data (which is actually a dev part from the contest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hcq1MI32WOTC",
        "outputId": "0ccc12ff-83e1-423d-aeea-3ec49385184f"
      },
      "source": [
        "test_data['trunctuation_bs'] = test_data['INPUT:source'].apply(lambda x: ' '.join(x.split()[:int(len(x.split())*0.6)])+'.')\n",
        "test_data.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>INPUT:source</th>\n",
              "      <th>OUTPUT:output</th>\n",
              "      <th>trunctuation_bs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>686</td>\n",
              "      <td>В Китае передача идёт в основном в кругу семьи, внутрибольничная передача в данной стране для инфекции не характерна.</td>\n",
              "      <td>В Китае обычно заражение происходит от родственников, а не из-за посещения больницы.</td>\n",
              "      <td>В Китае передача идёт в основном в кругу семьи, внутрибольничная.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>3229</td>\n",
              "      <td>За столетия потрясений численность этнических вавилонян в Южной Месопотамии сократилась, большинство населения там составляли халдеи.</td>\n",
              "      <td>За последние несколько столетий сократилось население вавилонян в Южной Месопотамии</td>\n",
              "      <td>За столетия потрясений численность этнических вавилонян в Южной Месопотамии.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>4178</td>\n",
              "      <td>Красный Крест сообщил, что не может начать работу в Карабахе из-за нарушений перемирия.</td>\n",
              "      <td>Работа красного креста не возможра в аарабахе по причине нарушения перемирия.</td>\n",
              "      <td>Красный Крест сообщил, что не может начать.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3146</th>\n",
              "      <td>9234</td>\n",
              "      <td>Туристическая индустрия Ирана серьёзно пострадала в результате ирано-иракской войны, однако в настоящее время возрождается.</td>\n",
              "      <td>Туриндустрия Ирана пострадала в результате ирано-иракской войны, но сейчас возрождается.</td>\n",
              "      <td>Туристическая индустрия Ирана серьёзно пострадала в результате ирано-иракской.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>2478</td>\n",
              "      <td>Гениальнейшим поэтом Польши и одновременно одним из великих мировых поэтов является Адам Мицкевич, признанный вождь польского романтизма.</td>\n",
              "      <td>Адам Мицкевич - всемирно известный польский поэт.\\r\\n</td>\n",
              "      <td>Гениальнейшим поэтом Польши и одновременно одним из великих мировых поэтов.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                                                                                                               INPUT:source                                                                             OUTPUT:output                                                                 trunctuation_bs\n",
              "225          686                      В Китае передача идёт в основном в кругу семьи, внутрибольничная передача в данной стране для инфекции не характерна.      В Китае обычно заражение происходит от родственников, а не из-за посещения больницы.               В Китае передача идёт в основном в кругу семьи, внутрибольничная.\n",
              "1073        3229      За столетия потрясений численность этнических вавилонян в Южной Месопотамии сократилась, большинство населения там составляли халдеи.       За последние несколько столетий сократилось население вавилонян в Южной Месопотамии    За столетия потрясений численность этнических вавилонян в Южной Месопотамии.\n",
              "1391        4178                                                    Красный Крест сообщил, что не может начать работу в Карабахе из-за нарушений перемирия.             Работа красного креста не возможра в аарабахе по причине нарушения перемирия.                                     Красный Крест сообщил, что не может начать.\n",
              "3146        9234                Туристическая индустрия Ирана серьёзно пострадала в результате ирано-иракской войны, однако в настоящее время возрождается.  Туриндустрия Ирана пострадала в результате ирано-иракской войны, но сейчас возрождается.  Туристическая индустрия Ирана серьёзно пострадала в результате ирано-иракской.\n",
              "844         2478  Гениальнейшим поэтом Польши и одновременно одним из великих мировых поэтов является Адам Мицкевич, признанный вождь польского романтизма.                                     Адам Мицкевич - всемирно известный польский поэт.\\r\\n     Гениальнейшим поэтом Польши и одновременно одним из великих мировых поэтов."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTGdTFZOXBdH"
      },
      "source": [
        "# Second baseline will be automatically generated by GPT-3\n",
        "\n",
        "I follow the procedure described in Sber repo\n",
        "# Finetune RuGPTs in megatron without deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1OzWZ6zqQv"
      },
      "source": [
        "!pip3 install transformers==3.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozJOYbK-11pk",
        "outputId": "674d6393-5227-48ff-8346-fcab84a73b97"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46Pk6DJ19Jk"
      },
      "source": [
        "!sh setup.sh\n",
        "!git clone  https://github.com/sberbank-ai/ru-gpts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SimLOj1xL46A"
      },
      "source": [
        "File: /usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py\n",
        "\n",
        "> `if version.parse(torch.__version__) <= version.parse(\"1.4.1\") or version.parse(torch.__version__) > version.parse(\"1.7.0\"):`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8di4sCoS0Pyw"
      },
      "source": [
        "## Download files\n",
        "\n",
        "Here we already need the translated data to tune a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qG_A1n0CiF"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1dB3X-Wx8qU_5nDG_pxAmLvo5H_sgnHrE\n",
        "! gdown https://drive.google.com/uc?id=1bJo8TagTGKa0uyppQRqsHrKHyYO5tcZc\n",
        "! gdown https://drive.google.com/uc?id=11lqipq6ggrgCk8bVxQ4-uuPVMCKN5ebU\n",
        "\n",
        "import pandas as pd\n",
        "df_dev_google = pd.read_csv('/content/wiki_dev_cleaned_translated_sd.csv')\n",
        "df_test_google = pd.read_csv('/content/wiki_test_cleaned_translated_sd.csv')\n",
        "df_train_google = pd.read_csv('/content/wiki_train_cleaned_translated_sd.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqW38Hni64xH"
      },
      "source": [
        "## Prepare data for parallel\n",
        "We use custom implementation of distributed dataset. For training and evaluating we should specify file `file.list` with list of paths to txt files. All files from `file.list` will be splitted between aviable GPUs. The logic of splitting is described by the following code:\n",
        "\n",
        "```python\n",
        "shard_size = len(files) // world_size\n",
        "shard_start = rank * shard_size\n",
        "shard_end = (rank + 1) * shard_size\n",
        "files = files[shard_start:shard_end]\n",
        "```\n",
        "\n",
        "For more details please see full code of dataset: `src.dataset_rugpt3.RuGpt3TextDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42JOCwCj4H3e"
      },
      "source": [
        "with open('train.txt', 'w+') as f:\n",
        "  for i,j in list(zip(df_train_google.target_x.values, df_train_google.target_y.values)):\n",
        "    f.write('<s>'+i+'\\n'+j+'\\n') # +'<\\s>'\n",
        "\n",
        "with open('valid.txt', 'w+') as f:\n",
        "  for i,j in list(zip(df_dev_google.target_x.values, df_dev_google.target_y.values)):\n",
        "    f.write('<s>'+i+'\\n'+j+'\\n') # +'<\\s>'\n",
        "\n",
        "!echo /content/train.txt > train.list\n",
        "!echo /content/valid.txt > valid.list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EF0JepF0S41"
      },
      "source": [
        "## Train\n",
        "Load model from Huggingface and finetune on essays.\n",
        "\n",
        "This will take arount ten minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3c6GmXqMSM8"
      },
      "source": [
        "! rm -r /content/model\n",
        "! rm -r /content/model_hf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHluAlFh0SJo",
        "outputId": "d29715c5-2a7f-4a0a-8b02-116e05a91b83"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/pretrain_gpt3.py \\\n",
        "  --train-data-path \"/content/train.list\" \\\n",
        "  --test-data-path \"/content/valid.list\" \\\n",
        "  --max-files-per-process 100 \\\n",
        "  --logging-dir=\"log\" \\\n",
        "  --save model \\\n",
        "  --load-huggingface sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --save-interval 1000 \\\n",
        "  --log-interval 100 \\\n",
        "  --eval-interval 1000 \\\n",
        "  --eval-iters 100 \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --batch-size 1 \\\n",
        "  --seq-length 512 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --train-iters 2000 \\\n",
        "  --resume-dataloader \\\n",
        "  --distributed-backend \"nccl\" \\\n",
        "  --lr 0.00015 \\\n",
        "  --lr-decay-style \"cosine\" \\\n",
        "  --lr-decay-iters 3200 \\\n",
        "  --clip-grad 0.5 \\\n",
        "  --warmup .004\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-13 16:44:11.667377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "using world size: 1 and model-parallel size: 1 \n",
            " > using dynamic loss scaling\n",
            "> initializing model parallel with size 1\n",
            "Pretrain GPT3 model\n",
            "arguments:\n",
            "  attention_dropout ............ 0.1\n",
            "  num_attention_heads .......... 12\n",
            "  hidden_size .................. 768\n",
            "  intermediate_size ............ None\n",
            "  num_layers ................... 12\n",
            "  layernorm_epsilon ............ 1e-05\n",
            "  hidden_dropout ............... 0.1\n",
            "  max_position_embeddings ...... 2048\n",
            "  vocab_size ................... 30522\n",
            "  deep_init .................... False\n",
            "  make_vocab_size_divisible_by . 8\n",
            "  cpu_optimizer ................ False\n",
            "  cpu_torch_adam ............... False\n",
            "  sparse_mode .................. all\n",
            "  fp16 ......................... False\n",
            "  fp32_embedding ............... False\n",
            "  fp32_layernorm ............... False\n",
            "  fp32_tokentypes .............. False\n",
            "  fp32_allreduce ............... False\n",
            "  hysteresis ................... 2\n",
            "  loss_scale ................... None\n",
            "  loss_scale_window ............ 1000\n",
            "  min_scale .................... 1\n",
            "  batch_size ................... 1\n",
            "  weight_decay ................. 0.01\n",
            "  checkpoint_activations ....... False\n",
            "  checkpoint_num_layers ........ 1\n",
            "  deepspeed_activation_checkpointing  False\n",
            "  clip_grad .................... 0.5\n",
            "  train_iters .................. 2000\n",
            "  log_interval ................. 100\n",
            "  logging_dir .................. log\n",
            "  exit_interval ................ None\n",
            "  seed ......................... 1234\n",
            "  reset_position_ids ........... False\n",
            "  reset_attention_mask ......... False\n",
            "  lr_decay_iters ............... 3200\n",
            "  lr_decay_style ............... cosine\n",
            "  lr ........................... 0.00015\n",
            "  min_lr ....................... 1e-06\n",
            "  warmup ....................... 0.004\n",
            "  save ......................... model\n",
            "  save_interval ................ 1000\n",
            "  no_save_optim ................ False\n",
            "  no_save_rng .................. False\n",
            "  load ......................... None\n",
            "  no_load_optim ................ False\n",
            "  log_memory ................... False\n",
            "  no_load_rng .................. False\n",
            "  load_huggingface ............. sberbank-ai/rugpt3small_based_on_gpt2\n",
            "  export_huggingface ........... None\n",
            "  huggingface_double_pos_embeddings  False\n",
            "  load_tag ..................... \n",
            "  cache_prefix ................. _\n",
            "  finetune ..................... False\n",
            "  resume_dataloader ............ True\n",
            "  distributed_backend .......... nccl\n",
            "  local_rank ................... None\n",
            "  eval_batch_size .............. None\n",
            "  eval_iters ................... 100\n",
            "  eval_interval ................ 1000\n",
            "  eval_seq_length .............. None\n",
            "  eval_max_preds_per_seq ....... None\n",
            "  overlapping_eval ............. 32\n",
            "  cloze_eval ................... False\n",
            "  eval_hf ...................... False\n",
            "  load_openai .................. False\n",
            "  temperature .................. 1.0\n",
            "  top_p ........................ 0.0\n",
            "  top_k ........................ 0\n",
            "  out_seq_length ............... 256\n",
            "  tg_token_name ................ token.txt\n",
            "  model_parallel_size .......... 1\n",
            "  shuffle ...................... False\n",
            "  train_data ................... None\n",
            "  use_npy_data_loader .......... False\n",
            "  train_data_path .............. /content/train.list\n",
            "  val_data_path ................ \n",
            "  test_data_path ............... /content/valid.list\n",
            "  input_data_sizes_file ........ sizes.txt\n",
            "  delim ........................ ,\n",
            "  text_key ..................... sentence\n",
            "  eval_text_key ................ None\n",
            "  valid_data ................... None\n",
            "  split ........................ 1000,1,1\n",
            "  test_data .................... None\n",
            "  overwrite_cache .............. False\n",
            "  lazy_loader .................. False\n",
            "  loose_json ................... False\n",
            "  presplit_sentences ........... False\n",
            "  num_workers .................. 2\n",
            "  tokenizer_path ............... None\n",
            "  cache_dir .................... None\n",
            "  use_tfrecords ................ False\n",
            "  seq_length ................... 512\n",
            "  max_files_per_process ........ 100\n",
            "  max_preds_per_seq ............ None\n",
            "  cuda ......................... True\n",
            "  rank ......................... 0\n",
            "  world_size ................... 1\n",
            "  dynamic_loss_scale ........... True\n",
            "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
            "Load tokenizer from sberbank-ai/rugpt3small_based_on_gpt2\n",
            "Load RuGPT3 Dataset from /content/train.list, 100 files per process\n",
            "WARNING:src.dataset_rugpt3:R0/1: Loading dataset /content/train.list\n",
            "WARNING:src.dataset_rugpt3:R0/1: Check filelist /content/train.list with root dir /content\n",
            "WARNING:src.dataset_rugpt3:R0/1: Shard [0, 1]\n",
            "WARNING:src.dataset_rugpt3:R0/1: Loaded 0/1 files\n",
            "WARNING:src.dataset_rugpt3:R0/1: Loaded 32201 examples, 16486912 tokens\n",
            "Load RuGPT3 Dataset from /content/valid.list, 100 files per process\n",
            "WARNING:src.dataset_rugpt3:R0/1: Loading dataset /content/valid.list\n",
            "WARNING:src.dataset_rugpt3:R0/1: Check filelist /content/valid.list with root dir /content\n",
            "WARNING:src.dataset_rugpt3:R0/1: Shard [0, 1]\n",
            "  0% 0/1 [00:00<?, ?it/s]WARNING:src.dataset_rugpt3:R0/1: Loaded 0/1 files\n",
            "100% 1/1 [00:00<00:00, 95.88it/s]\n",
            "WARNING:src.dataset_rugpt3:R0/1: Loaded 100 examples, 51200 tokens\n",
            "> padded vocab (size: 50257) with 7 dummy tokens (new size: 50264)\n",
            "> end-of-document token: 0\n",
            "building GPT3 model ...\n",
            "Load huggingface model from sberbank-ai/rugpt3small_based_on_gpt2 \n",
            "Loaded huggingface model <class 'src.model.gpt3_modeling.GPT3Model'>\n",
            " > number of parameters on model parallel rank 0: 125231616\n",
            "Optimizer = FusedAdam\n",
            "learning rate decaying cosine\n",
            "Resume train set from iteration 0\n",
            "--Start training loop--\n",
            " iteration      100/    2000 | elapsed time per iteration (ms): 364.0 | learning rate 1.497E-04 | lm loss 2.4713 | perplexity 11.8376 |\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:375: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:383: FutureWarning: torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved\n",
            "  FutureWarning)\n",
            "after 100 iterations memory (MB) | allocated: 1927.36083984375 | max allocated: 3471.7802734375 | cached: 4120.0 | max cached: 4120.0\n",
            "time (ms) | forward: 113.61 | backward: 223.20 | allreduce: 22.84 | optimizer: 26.19 | data loader: 0.25\n",
            " iteration      200/    2000 | elapsed time per iteration (ms): 365.3 | learning rate 1.487E-04 | lm loss 2.6057 | perplexity 13.5402 |\n",
            "time (ms) | forward: 114.20 | backward: 223.98 | allreduce: 22.83 | optimizer: 26.12 | data loader: 0.22\n",
            " iteration      300/    2000 | elapsed time per iteration (ms): 365.2 | learning rate 1.470E-04 | lm loss 2.5117 | perplexity 12.3260 |\n",
            "time (ms) | forward: 114.03 | backward: 224.23 | allreduce: 22.82 | optimizer: 26.08 | data loader: 0.22\n",
            " iteration      400/    2000 | elapsed time per iteration (ms): 364.9 | learning rate 1.446E-04 | lm loss 2.5688 | perplexity 13.0501 |\n",
            "time (ms) | forward: 113.95 | backward: 223.96 | allreduce: 22.79 | optimizer: 26.08 | data loader: 0.22\n",
            " iteration      500/    2000 | elapsed time per iteration (ms): 365.0 | learning rate 1.416E-04 | lm loss 2.5924 | perplexity 13.3620 |\n",
            "time (ms) | forward: 113.96 | backward: 224.06 | allreduce: 22.81 | optimizer: 26.08 | data loader: 0.25\n",
            " iteration      600/    2000 | elapsed time per iteration (ms): 365.1 | learning rate 1.379E-04 | lm loss 2.5177 | perplexity 12.4002 |\n",
            "time (ms) | forward: 114.02 | backward: 224.02 | allreduce: 22.84 | optimizer: 26.09 | data loader: 0.23\n",
            " iteration      700/    2000 | elapsed time per iteration (ms): 365.2 | learning rate 1.336E-04 | lm loss 2.5171 | perplexity 12.3926 |\n",
            "time (ms) | forward: 114.04 | backward: 224.13 | allreduce: 22.77 | optimizer: 26.08 | data loader: 0.23\n",
            " iteration      800/    2000 | elapsed time per iteration (ms): 365.3 | learning rate 1.287E-04 | lm loss 2.4661 | perplexity 11.7761 |\n",
            "time (ms) | forward: 114.09 | backward: 224.20 | allreduce: 22.80 | optimizer: 26.08 | data loader: 0.22\n",
            " iteration      900/    2000 | elapsed time per iteration (ms): 365.4 | learning rate 1.233E-04 | lm loss 2.4514 | perplexity 11.6040 |\n",
            "time (ms) | forward: 114.00 | backward: 224.40 | allreduce: 22.81 | optimizer: 26.07 | data loader: 0.24\n",
            " iteration     1000/    2000 | elapsed time per iteration (ms): 365.3 | learning rate 1.174E-04 | lm loss 2.4829 | perplexity 11.9760 |\n",
            "time (ms) | forward: 114.01 | backward: 224.28 | allreduce: 22.77 | optimizer: 26.08 | data loader: 0.21\n",
            "global rank 0 is saving checkpoint at iteration    1000 to model/iter_0001000/mp_rank_00/model_optim_rng.pt\n",
            "  successfully saved model/iter_0001000/mp_rank_00/model_optim_rng.pt\n",
            " iteration     1100/    2000 | elapsed time per iteration (ms): 437.3 | learning rate 1.112E-04 | lm loss 2.4063 | perplexity 11.0932 |\n",
            "time (ms) | forward: 114.65 | backward: 224.98 | allreduce: 22.70 | optimizer: 26.08 | data loader: 0.24\n",
            " iteration     1200/    2000 | elapsed time per iteration (ms): 365.4 | learning rate 1.046E-04 | lm loss 2.4018 | perplexity 11.0430 |\n",
            "time (ms) | forward: 114.03 | backward: 224.36 | allreduce: 22.78 | optimizer: 26.09 | data loader: 0.22\n",
            " iteration     1300/    2000 | elapsed time per iteration (ms): 365.3 | learning rate 9.767E-05 | lm loss 2.4416 | perplexity 11.4918 |\n",
            "time (ms) | forward: 114.04 | backward: 224.33 | allreduce: 22.77 | optimizer: 26.08 | data loader: 0.21\n",
            " iteration     1400/    2000 | elapsed time per iteration (ms): 365.5 | learning rate 9.055E-05 | lm loss 2.3669 | perplexity 10.6646 |\n",
            "time (ms) | forward: 114.13 | backward: 224.39 | allreduce: 22.84 | optimizer: 26.08 | data loader: 0.23\n",
            " iteration     1500/    2000 | elapsed time per iteration (ms): 365.1 | learning rate 8.329E-05 | lm loss 2.4403 | perplexity 11.4763 |\n",
            "time (ms) | forward: 114.11 | backward: 223.93 | allreduce: 22.76 | optimizer: 26.10 | data loader: 0.23\n",
            " iteration     1600/    2000 | elapsed time per iteration (ms): 365.2 | learning rate 7.594E-05 | lm loss 2.4464 | perplexity 11.5464 |\n",
            "time (ms) | forward: 114.02 | backward: 224.24 | allreduce: 22.79 | optimizer: 26.07 | data loader: 0.24\n",
            " iteration     1700/    2000 | elapsed time per iteration (ms): 365.3 | learning rate 6.859E-05 | lm loss 2.3405 | perplexity 10.3860 |\n",
            "time (ms) | forward: 114.08 | backward: 224.17 | allreduce: 22.80 | optimizer: 26.08 | data loader: 0.23\n",
            " iteration     1800/    2000 | elapsed time per iteration (ms): 365.4 | learning rate 6.129E-05 | lm loss 2.3259 | perplexity 10.2354 |\n",
            "time (ms) | forward: 114.14 | backward: 224.18 | allreduce: 22.79 | optimizer: 26.12 | data loader: 0.24\n",
            " iteration     1900/    2000 | elapsed time per iteration (ms): 365.8 | learning rate 5.413E-05 | lm loss 2.3778 | perplexity 10.7815 |\n",
            "time (ms) | forward: 114.16 | backward: 224.50 | allreduce: 22.91 | optimizer: 26.10 | data loader: 0.26\n",
            " iteration     2000/    2000 | elapsed time per iteration (ms): 365.4 | learning rate 4.717E-05 | lm loss 2.3271 | perplexity 10.2483 |\n",
            "time (ms) | forward: 114.14 | backward: 224.25 | allreduce: 22.87 | optimizer: 26.10 | data loader: 0.23\n",
            "global rank 0 is saving checkpoint at iteration    2000 to model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "  successfully saved model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "global rank 0 is saving checkpoint at iteration    2000 to model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "  successfully saved model/iter_0002000/mp_rank_00/model_optim_rng.pt\n",
            "Evaluating iter 100/100\n",
            "-----------------------------------------------------------------------------------------\n",
            " validation loss at the end of training for test data | LM loss: 2.5496 | LM PPL: 12.802\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALvcD5SE8RtP"
      },
      "source": [
        "At the end of training output should be something like this:\n",
        "\n",
        "\"-----------------------------------------------------------------------------------------\n",
        "\n",
        " validation loss at the end of training for test data | LM loss: 2.7927 | LM PPL: 16.325\n",
        " \n",
        "-----------------------------------------------------------------------------------------\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAoOxKMB--pa"
      },
      "source": [
        "# ! cp -r /content/model /content/drive/MyDrive/SImplification_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HmKilrb8lQm"
      },
      "source": [
        "## Generate\n",
        "\n",
        "Load pretrained model from dir and generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAH-WpCG8lmG"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/generate_samples.py \\\n",
        "  --load /content/drive/MyDrive/SImplification_models/model \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --batch-size 1 \\\n",
        "  --seq-length 50 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --distributed-backend \"nccl\" \\\n",
        "  --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --no-load-optim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCapfDfeBq0x"
      },
      "source": [
        "### Convert checkpoint to Huggingface format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK2yzulB5GWG"
      },
      "source": [
        "# /content/model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnhIyqd-Eeo"
      },
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!python ru-gpts/convert2huggingface.py \\\n",
        "  --load model \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 12 \\\n",
        "  --hidden-size 768 \\\n",
        "  --num-attention-heads 12 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --no-load-optim \\\n",
        "  --export-huggingface /content/drive/MyDrive/SImplification_models/model_hf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIfhv7KvDKQi"
      },
      "source": [
        "!ls model_hf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRlEwlPdE0L8"
      },
      "source": [
        "#### Test load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x1zLGCiEwkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907acd65-3b00-455d-97f9-d2e6cf471494"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def create_model_and_tok(\n",
        "        model_name=\"/content/drive/MyDrive/SImplification_models/model_hf\" #/content/drive/MyDrive/SImplification_models/\n",
        "):\n",
        "    print('loading from {}'.format(model_name))\n",
        "    gpt_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    gpt_model.cuda()\n",
        "    gpt_model.eval()\n",
        "    gpt_tokenizer.padding_side = \"left\"\n",
        "    gpt_tokenizer.pad_token = '[PAD]'\n",
        "    gpt_tokenizer.encoder['[PAD]'] = 50256\n",
        "    gpt_model.config.pad_token_id = gpt_model.config.eos_token_id\n",
        "    return gpt_model, gpt_tokenizer\n",
        "\n",
        "\n",
        "def batch_generator(\n",
        "        list_of_sentences,\n",
        "        size=16\n",
        "):\n",
        "    num_batch = len(list_of_sentences)//size\n",
        "    for index in range(num_batch):\n",
        "        yield list_of_sentences[index*size:(index+1)*size]\n",
        "    yield list_of_sentences[num_batch*size:]\n",
        "\n",
        "\n",
        "def get_outputs(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        list_of_sentences,\n",
        "        seq_len=11, \n",
        "        batch_size=16\n",
        "):\n",
        "    result = []\n",
        "    for batch in batch_generator(list_of_sentences, batch_size):\n",
        "        max_length = len(max(list_of_sentences, key=lambda x: x.split()).split()) + 1\n",
        "        encodings_dict = tokenizer.batch_encode_plus(batch, max_length=max_length, pad_to_max_length=True, add_special_tokens=False)\n",
        "        input_ids = torch.tensor(encodings_dict['input_ids']).cuda()\n",
        "        attn_mask = torch.tensor(encodings_dict['attention_mask']).cuda()\n",
        "\n",
        "        outputs = model.generate(input_ids,\n",
        "                                 attention_mask=attn_mask,\n",
        "                                 do_sample=True,\n",
        "                                 max_length=40, #1000+ max_length,\n",
        "                                 top_k=10,\n",
        "                                 top_p=0.95,\n",
        "                                 repetition_penalty=5.0,\n",
        "                                 num_return_sequences=1)\n",
        "                                 # no_repeat_ngram_size=3)\n",
        "\n",
        "        outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "        outputs = [text[:re.search(r'<\\\\?s?>?', text).start()].strip() if  re.search(r'<\\\\?s?>?', text) is not None else text.strip() for text  in outputs]\n",
        "        result.extend(outputs)\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prompt_text = [\n",
        "        \"Британская транспортная комиссия (BTC) была создана послевоенным лейбористским правительством Клемента Эттли в рамках его программы национализации для надзора за железными дорогами, каналами и автомобильными грузовыми перевозками в Великобритании (в Северной Ирландии было отдельное транспортное управление Ольстера).\",\n",
        "        \"Отношения Фицджеральда с Гранцем еще больше укрепились, когда он стал ее менеджером, хотя прошло почти десять лет, прежде чем он смог записать ее на одном из своих многочисленных лейблов.\",\n",
        "        \"Они вымерли на материке, а оставшиеся популяции были ограничены 32 прибрежными островами до первого выпуска с материка в сильно огороженный и находящийся под наблюдением заповедник Карори в 2005 году.\",\n",
        "        \"Вместе с Bryozoa и Brachiopoda форониды принадлежат к лофофоратам, которые иногда рассматриваются как один тип.\"]\n",
        "\n",
        "    # prompt_text = [\"Python популярен среди индивидуальных разработчиков, но также используется крупными компаниями в достаточно серьёзных продуктах, ориентированных на получение прибыли.\",\n",
        "    #                \"Официальной столицей государства, согласно конституции Нидерландов, является Амстердам, где монарх приносит присягу на верность Конституции.\",\n",
        "    #                \"После первого неудачного сражения Джексон отступил на юг по долине, затем атаковал и разбил отряд Фримонта.\"]\n",
        "    # /mnt/sdb1/data/aizhevskaya/ft_model_new/checkpoint-3000\n",
        "    # \"/mnt/sdb1/data/aizhevskaya/ft_model_new/three_epoch\"\n",
        "    # /mnt/sdb1/data/aizhevskaya/ft_model/checkpoint-30000\n",
        "    # \"/mnt/sdb1/data/aizhevskaya/ft_model_new/simpl1\"\n",
        "    model, tokenizer = create_model_and_tok()\n",
        "    # perplexity = tensor(16.6698)\n",
        "    texts = get_outputs(model, tokenizer, prompt_text)\n",
        "    for text in texts:\n",
        "        print(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading from /content/drive/MyDrive/SImplification_models/model_hf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
            "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
            "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
            "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Британская транспортная комиссия (BTC) была создана послевоенным лейбористским правительством Клемента Эттли в рамках его программы национализации, а затем преобразована из Британской транспортной комиссии\n",
            "Отношения Фицджеральда с Гранцем еще больше укрепились, когда он стал ее менеджером, хотя прошло почти десять лет, прежде чем она перешла на работу в Microsoft.\n",
            "Они вымерли на материке, а оставшиеся популяции были ограничены 32 прибрежными островами до первого выпуска с материка в сильно огороженный и густонаселенный регион Северной Америки.\n",
            "Вместе с Bryozoa и Brachiopoda форониды принадлежат к лофофоратам, которые иногда рассматриваются как этологи.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Idw0FxcVLk"
      },
      "source": [
        "# Look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "jbRGfZREa3i2",
        "outputId": "b1ecbf54-4ac9-4563-f98f-b88fc9aa3c82"
      },
      "source": [
        "model, tokenizer = create_model_and_tok()\n",
        "texts = get_outputs(model, tokenizer, test_data['INPUT:source'], batch_size=32)\n",
        "test_data['gpt_bs'] = texts\n",
        "test_data.sample(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>INPUT:source</th>\n",
              "      <th>OUTPUT:output</th>\n",
              "      <th>trunctuation_bs</th>\n",
              "      <th>gpt_bs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2595</th>\n",
              "      <td>7703</td>\n",
              "      <td>Рамзан Кадыров назвал возрождение многонационального сообщества республики одной из приоритетных задач нового руководства республики.</td>\n",
              "      <td>Рамзан Кадыров считает, что главная задача руководства - возродить многонациональное общество республики.</td>\n",
              "      <td>Рамзан Кадыров назвал возрождение многонационального сообщества республики одной.</td>\n",
              "      <td>Рамзан Кадыров назвал возрождение многонационального сообщества республики одной из приоритетных задач.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>9354</td>\n",
              "      <td>Улучшение оксигенации лёгких, наблюдаемое при интенсификации дыхания в разрежённом воздухе горных курортов, способствует торможению роста и размножения микобактерий.</td>\n",
              "      <td>Улучшение обогащения лёгких кислородом, которое наблюдается при усилении функций дыхания в разрежённом воздухе горных курортов, способствует также размножению и росту микобактерий.</td>\n",
              "      <td>Улучшение оксигенации лёгких, наблюдаемое при интенсификации дыхания в разрежённом воздухе.</td>\n",
              "      <td>Улучшение оксигенации лёгких, наблюдаемое при интенсификации окиснения урана (AO), является важным механизмом очистки и удаления отходов в легких.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>2391</td>\n",
              "      <td>Вторично одичавшие кошки часто живут уединённо и охотятся в одиночку, но иногда образуют небольшие колонии из нескольких самок с котятами.</td>\n",
              "      <td>Как правило, вторично одичавшие кошки и живут и охотятся в одиночку, но бывает, что иногда они образуют отдельные поселения, в которые входят несколько самок и их котята.</td>\n",
              "      <td>Вторично одичавшие кошки часто живут уединённо и охотятся в одиночку, но иногда.</td>\n",
              "      <td>Вторично одичавшие кошки часто живут уединённо и охотятся в дикой природе.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                                                                                                                                           INPUT:source                                                                                                                                                                         OUTPUT:output                                                                              trunctuation_bs                                                                                                                                              gpt_bs\n",
              "2595        7703                                  Рамзан Кадыров назвал возрождение многонационального сообщества республики одной из приоритетных задач нового руководства республики.                                                                             Рамзан Кадыров считает, что главная задача руководства - возродить многонациональное общество республики.            Рамзан Кадыров назвал возрождение многонационального сообщества республики одной.                                             Рамзан Кадыров назвал возрождение многонационального сообщества республики одной из приоритетных задач.\n",
              "3175        9354  Улучшение оксигенации лёгких, наблюдаемое при интенсификации дыхания в разрежённом воздухе горных курортов, способствует торможению роста и размножения микобактерий.  Улучшение обогащения лёгких кислородом, которое наблюдается при усилении функций дыхания в разрежённом воздухе горных курортов, способствует также размножению и росту микобактерий.  Улучшение оксигенации лёгких, наблюдаемое при интенсификации дыхания в разрежённом воздухе.  Улучшение оксигенации лёгких, наблюдаемое при интенсификации окиснения урана (AO), является важным механизмом очистки и удаления отходов в легких.\n",
              "808         2391                             Вторично одичавшие кошки часто живут уединённо и охотятся в одиночку, но иногда образуют небольшие колонии из нескольких самок с котятами.           Как правило, вторично одичавшие кошки и живут и охотятся в одиночку, но бывает, что иногда они образуют отдельные поселения, в которые входят несколько самок и их котята.              Вторично одичавшие кошки часто живут уединённо и охотятся в одиночку, но иногда.                                                                          Вторично одичавшие кошки часто живут уединённо и охотятся в дикой природе."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hCUeqncxRQ"
      },
      "source": [
        "## SAVING BASELINES...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR88v8W_gLYQ"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "## cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AwyL9m1go5m"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel, AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "config = AutoConfig.from_pretrained(\"DeepPavlov/rubert-base-cased\") # \"roberta-base\" 'xlm-mlm-100-1280' 'xlm-roberta-base' 'bert-base-multilingual-cased'\n",
        "config.output_hidden_states = True\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"DeepPavlov/rubert-base-cased\", config=config)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64xt-k-ogw8Q"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# shape should be [1, something (768, ex)]\n",
        "\n",
        "# import numpy as np\n",
        "# def cs(a, b):\n",
        "#   return (a @ b.T)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "def calc_cos_sim(df, model,tok, x, y, column_name):\n",
        "    Cos_sim= []\n",
        "    for index, row in df.iterrows():\n",
        "        \n",
        "        # original\n",
        "          sentence_A = tok.encode(row[x], padding='max_length', max_length=50, truncation=True, return_tensors='pt')\n",
        "          sentence_A = sentence_A.to(device)\n",
        "          output = model(sentence_A)\n",
        "          sent_emb = output[-1][0]\n",
        "          emb_source = sent_emb.mean(axis=1)\n",
        "          emb_source = emb_source.cpu().detach().numpy()\n",
        "\n",
        "          sentence_B = tok.encode(row[y], padding='max_length', max_length=50, truncation=True, return_tensors='pt')\n",
        "          sentence_B = sentence_B.to(device)\n",
        "          output = model(sentence_B)\n",
        "          sent_emb = output[-1][0]\n",
        "          emb_target= sent_emb.mean(axis=1)\n",
        "          emb_target = emb_target.cpu().detach().numpy()\n",
        "\n",
        "          cos_val = cosine_similarity(emb_source.reshape(emb_source.shape[0], -1), emb_target.reshape(emb_target.shape[0], -1))[0][0]\n",
        "          Cos_sim.append(cos_val)\n",
        "    df[column_name] = Cos_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWomOnnbhwyW"
      },
      "source": [
        "calc_cos_sim(test_data, model, tok, 'INPUT:source', 'OUTPUT:output', 'cos_sim_ref')\n",
        "calc_cos_sim(test_data, model, tok, 'INPUT:source', 'trunctuation_bs', 'cos_sim_tr_bs')\n",
        "calc_cos_sim(test_data, model, tok, 'INPUT:source', 'gpt_bs', 'cos_sim_gpt_bs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "CG-j3eZFjcM9",
        "outputId": "dbc47cee-0bb1-41c6-c500-6b6daa5dc3ae"
      },
      "source": [
        "test_data.to_csv('/content/drive/MyDrive/SImplification_models/test_baselines.csv', index=False, sep='\\t')\n",
        "test_data.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>INPUT:source</th>\n",
              "      <th>OUTPUT:output</th>\n",
              "      <th>trunctuation_bs</th>\n",
              "      <th>gpt_bs</th>\n",
              "      <th>cos_sim_ref</th>\n",
              "      <th>cos_sim_tr_bs</th>\n",
              "      <th>cos_sim_gpt_bs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>14 декабря 1944 года рабочий посёлок Ички был ...</td>\n",
              "      <td>14 декабря 1944 года рабочий посёлок Ички пере...</td>\n",
              "      <td>14 декабря 1944 года рабочий посёлок Ички был ...</td>\n",
              "      <td>14 декабря 1944 года рабочий посёлок Ички был ...</td>\n",
              "      <td>0.924978</td>\n",
              "      <td>0.928283</td>\n",
              "      <td>0.970611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменени...</td>\n",
              "      <td>В 1960 году вышла модель 172А. Отличие в хвост...</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменения...</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменения...</td>\n",
              "      <td>0.962903</td>\n",
              "      <td>0.954120</td>\n",
              "      <td>0.957274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменени...</td>\n",
              "      <td>В выпущенной в 1960 году модель имела изменени...</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменения...</td>\n",
              "      <td>1960 году была выпущена модель 172A. Изменения...</td>\n",
              "      <td>0.950816</td>\n",
              "      <td>0.954120</td>\n",
              "      <td>0.970414</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... cos_sim_gpt_bs\n",
              "0           3  ...       0.970611\n",
              "1           4  ...       0.957274\n",
              "2           5  ...       0.970414\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4MKK6qsQmC"
      },
      "source": [
        "## BLEU SARI (only SARI so far)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcTck-T2zda3"
      },
      "source": [
        "# import pandas as pd\n",
        "# test_data = pd.read_csv('/content/drive/MyDrive/MT_sentence_simpl/test_baselines.csv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHs4_tbWsbYk"
      },
      "source": [
        "! git clone https://github.com/feralvam/easse\n",
        "! git clone https://github.com/Andoree/sent_simplification.git\n",
        "%cp /content/sent_simplification/sari.py /content/easse/easse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwYA6b-mtwmb"
      },
      "source": [
        "%cd easse\n",
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWv3G7eSwIdc"
      },
      "source": [
        "! mkdir /content/drive/MyDrive/SImplification_models/easse_baseline_res\n",
        "! mkdir prepared_data\n",
        "!mkdir preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EavdIO8pzUtc"
      },
      "source": [
        "Prepare data for SARI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4EVuZ9DseB4",
        "outputId": "cacc5986-5475-49fa-f363-7565ca981ac0"
      },
      "source": [
        "! python /content/sent_simplification/refs_to_easse_format.py \\\n",
        "--input_path /content/drive/MyDrive/SImplification_models/test_baselines.csv \\\n",
        "--output_dataset_name test_ref_data \\\n",
        "--src_column \"INPUT:source\" \\\n",
        "--trg_column \"OUTPUT:output\" \\\n",
        "--output_dir /content/prepared_data"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "3406\n",
            "3406\n",
            "Overall number of references: 3406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pDCJz1pirJR"
      },
      "source": [
        "with open('/content/prepared_data/gpt_bs_pred.hyp', 'w+') as f:\n",
        "  for i in test_data['gpt_bs'].values:\n",
        "      f.write(re.sub(r'[\\t\\n\\r\\f\\v]{1,5}', \"\",i)+'\\n')\n",
        "\n",
        "with open('/content/prepared_data/trunctuation_bs_pred.hyp', 'w+') as f:\n",
        "  for i in test_data['trunctuation_bs'].values:\n",
        "      f.write(i+'\\n')\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1s02bRUgNdW"
      },
      "source": [
        "with open('/content/prepared_data/dst_sent.txt', 'w+') as f:\n",
        "  for i in test_data['OUTPUT:output'].values:\n",
        "      f.write(re.sub(r'[\\t\\n\\r\\f\\v]{1,10}', \"\",i)+'\\n')\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sym2n-j0IrNB"
      },
      "source": [
        "! mkdir /content/preds/test\n",
        "%cp /content/preds/pred_tr_bs.tok /content/preds/test/pred_tr_bs.raw # ???"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0ew4HltgSm"
      },
      "source": [
        "# EXPERIMENT\n",
        "# test_data['INPUT:source'] = test_data['INPUT:source'].apply(lambda x: re.sub(r'[\\t\\n\\r\\f\\v]{1,10}', \"\",x))\n",
        "# test_data['OUTPUT:output'] = test_data['OUTPUT:output'].apply(lambda x: re.sub(r'[\\t\\n\\r\\f\\v]{1,10}', \"\",x))\n",
        "# test_data.to_csv('test_data_new.csv', index=False)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moyh8Ip1N1s2"
      },
      "source": [
        "## SARI\n",
        "1 Trunctuation Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWLp0Z6wubkQ"
      },
      "source": [
        "# ! wget https://github.com/dialogue-evaluation/RuSimpleSentEval/blob/main/dev_sents.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUW3NI16sgbW",
        "outputId": "098cf937-cb0a-4759-c43b-ee1eec1a98ff"
      },
      "source": [
        "! easse evaluate \\\n",
        "--test_set custom \\\n",
        "--metrics sari \\\n",
        "--refs_sents_paths /content/prepared_data/test_ref_data.ref.0,/content/prepared_data/test_ref_data.ref.1,/content/prepared_data/test_ref_data.ref.2,/content/prepared_data/test_ref_data.ref.3,/content/prepared_data/test_ref_data.ref.4 \\\n",
        "--orig_sents_path /content/prepared_data/test_ref_data.src \\\n",
        "--sys_sents_path /content/prepared_data/trunctuation_bs_pred.hyp -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sari': 32.02, 'quality_estimation': {'Compression ratio': 0.602, 'Sentence splits': 0.994, 'Levenshtein similarity': 0.348, 'Exact copies': 0.0, 'Additions proportion': 0.496, 'Deletions proportion': 0.885, 'Lexical complexity score': 10.431}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTmj0bBMOsNt"
      },
      "source": [
        "2 GPT baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkBwFRdbOvS5",
        "outputId": "71f3aa47-df4b-45c8-a879-7ad6876f7d03"
      },
      "source": [
        "! easse evaluate \\\n",
        "--test_set custom \\\n",
        "--metrics sari \\\n",
        "--refs_sents_paths /content/prepared_data/test_ref_data.ref.0,/content/prepared_data/test_ref_data.ref.1,/content/prepared_data/test_ref_data.ref.2,/content/prepared_data/test_ref_data.ref.3,/content/prepared_data/test_ref_data.ref.4 \\\n",
        "--orig_sents_path /content/prepared_data/test_ref_data.src \\\n",
        "--sys_sents_path /content/prepared_data/gpt_bs_pred.hyp -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sari': 32.52, 'quality_estimation': {'Compression ratio': 0.944, 'Sentence splits': 1.01, 'Levenshtein similarity': 0.371, 'Exact copies': 0.0, 'Additions proportion': 0.732, 'Deletions proportion': 0.798, 'Lexical complexity score': 10.206}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw_cUF7R05s_"
      },
      "source": [
        "#. ,$REFERENCES_DIR/$DATASET_NAME.ref.5,$REFERENCES_DIR/$DATASET_NAME.ref.6,$REFERENCES_DIR/$DATASET_NAME.ref.7,$REFERENCES_DIR/$DATASET_NAME.ref.8,$REFERENCES_DIR/$DATASET_NAME.ref.9,$REFERENCES_DIR/$DATASET_NAME.ref.10,$REFERENCES_DIR/$DATASET_NAME.ref.11,$REFERENCES_DIR/$DATASET_NAME.ref.12,$REFERENCES_DIR/$DATASET_NAME.ref.13,$REFERENCES_DIR/$DATASET_NAME.ref.14,$REFERENCES_DIR/$DATASET_NAME.ref.15,$REFERENCES_DIR/$DATASET_NAME.ref.16,$REFERENCES_DIR/$DATASET_NAME.ref.17,$REFERENCES_DIR/$DATASET_NAME.ref.18,$REFERENCES_DIR/$DATASET_NAME.ref.19,$REFERENCES_DIR/$DATASET_NAME.ref.20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4hE-F8vzRHe"
      },
      "source": [
        "### BLEU??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPZbKeVBpo9b",
        "outputId": "cff9297b-cdd0-4bd3-d8ba-f74256f4b202"
      },
      "source": [
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='ru')\n",
        "\n",
        "\n",
        "# Open the test dataset human translation file and detokenize the references\n",
        "refs = []\n",
        "\n",
        "with open(\"/content/prepared_data/dst_sent.txt\") as test:\n",
        "    for line in test: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        refs.append(line)\n",
        "    \n",
        "print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "with open(\"/content/prepared_data/trunctuation_bs_pred.hyp\") as pred:  \n",
        "    for line in pred: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        preds.append(line)\n",
        "\n",
        "print(\"MTed 1st sentence:\", preds[0])    \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: 14 декабря 1944 года рабочий посёлок Ички переименован в Советский.\n",
            "MTed 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок.\n",
            "12.220596636706833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpJCEP9Cvjvc",
        "outputId": "8b1329c3-c800-4790-ad95-6f3686b01145"
      },
      "source": [
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='ru')\n",
        "\n",
        "\n",
        "# Open the test dataset human translation file and detokenize the references\n",
        "refs = []\n",
        "\n",
        "with open(\"/content/prepared_data/src_sent.txt\") as test:\n",
        "    for line in test: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        refs.append(line)\n",
        "    \n",
        "print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "with open(\"/content/prepared_data/trunctuation_bs_pred.hyp\") as pred:  \n",
        "    for line in pred: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        preds.append(line)\n",
        "\n",
        "print(\"MTed 1st sentence:\", preds[0])    \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок Советский, после чего поселковый совет стал называться Советским.\n",
            "MTed 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок.\n",
            "48.061069407227485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUu8n7lQve2p",
        "outputId": "f5caeed3-0aff-4084-903f-7d1f21b24a11"
      },
      "source": [
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='ru')\n",
        "\n",
        "\n",
        "# Open the test dataset human translation file and detokenize the references\n",
        "refs = []\n",
        "\n",
        "with open(\"/content/prepared_data/dst_sent.txt\") as test:\n",
        "    for line in test: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        refs.append(line)\n",
        "    \n",
        "print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "with open(\"/content/prepared_data/gpt_bs_pred.hyp\") as pred:  \n",
        "    for line in pred: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        preds.append(line)\n",
        "\n",
        "print(\"MTed 1st sentence:\", preds[0])    \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: 14 декабря 1944 года рабочий посёлок Ички переименован в Советский.\n",
            "MTed 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок Советский, который тогда назывался Рабоче-Крестьянской республикой.\n",
            "9.86552026729868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aghhvpguqREK",
        "outputId": "6e8886aa-f28f-4898-9351-ad7b7e4edc14"
      },
      "source": [
        "import sacrebleu\n",
        "from sacremoses import MosesDetokenizer\n",
        "md = MosesDetokenizer(lang='ru')\n",
        "\n",
        "\n",
        "# Open the test dataset human translation file and detokenize the references\n",
        "refs = []\n",
        "\n",
        "with open(\"/content/prepared_data/src_sent.txt\") as test:\n",
        "    for line in test: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        refs.append(line)\n",
        "    \n",
        "print(\"Reference 1st sentence:\", refs[0])\n",
        "\n",
        "refs = [refs]  # Yes, it is a list of list(s) as required by sacreBLEU\n",
        "\n",
        "\n",
        "# Open the translation file by the NMT model and detokenize the predictions\n",
        "preds = []\n",
        "\n",
        "with open(\"/content/prepared_data/gpt_bs_pred.hyp\") as pred:  \n",
        "    for line in pred: \n",
        "        line = line.strip().split() \n",
        "        line = md.detokenize(line) \n",
        "        preds.append(line)\n",
        "\n",
        "print(\"MTed 1st sentence:\", preds[0])    \n",
        "\n",
        "\n",
        "# Calculate and print the BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок Советский, после чего поселковый совет стал называться Советским.\n",
            "MTed 1st sentence: 14 декабря 1944 года рабочий посёлок Ички был переименован в рабочий посёлок Советский, который тогда назывался Рабоче-Крестьянской республикой.\n",
            "56.42809934158597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFAC_0yHUskd"
      },
      "source": [
        "# Flesch-Kincard Grade Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRtL2fddiTK"
      },
      "source": [
        "http://ceur-ws.org/Vol-2780/paper2.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkZ3XKz0SW9"
      },
      "source": [
        "textstat.set_lang('ru')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbeZVvlc1g9U"
      },
      "source": [
        "### Original Sentences and their simplifications scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umzc0TU81shg"
      },
      "source": [
        "Flesh Kincaid Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJtr_NVY0jV9",
        "outputId": "0ac74fa6-23f2-4c69-d60b-8ef472bd79a5"
      },
      "source": [
        "a, b = test_data['INPUT:source'].values, test_data['OUTPUT:output'].values\n",
        "orig = sum(textstat.flesch_kincaid_grade(i) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.flesch_kincaid_grade(i) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.591720493247266, 19.4141808573106)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPjhTWR31xP4"
      },
      "source": [
        "Syllables number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN5eQ20_4uPj",
        "outputId": "c55de607-e437-41fc-82d7-bd2beac309bc"
      },
      "source": [
        "orig = sum(textstat.syllable_count(i) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.syllable_count(i) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46.53758073987082, 31.318261890780974)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pOJNEnM10AS"
      },
      "source": [
        "Words number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bebD0MF41AV",
        "outputId": "bf676fad-50a3-44b4-fc04-8ee20f8be0e2"
      },
      "source": [
        "orig = sum(textstat.lexicon_count(i, removepunct=True) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.lexicon_count(i, removepunct=True) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.69583088667058, 12.413681738109219)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtA4D-_416NO"
      },
      "source": [
        "### Trunctuation baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gigLl4PN1-TT"
      },
      "source": [
        "Flesh Kincaid Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8RCjOIc246Z",
        "outputId": "cbc582f3-0979-4b5c-a7bd-e2fc26ff5ea4"
      },
      "source": [
        "a, b = test_data['trunctuation_bs'].values, test_data['gpt_bs'].values\n",
        "tr_bs = sum(textstat.flesch_kincaid_grade(i) for i in a)/len(a)\n",
        "gpt_bs = sum(textstat.flesch_kincaid_grade(i) for i in b)/len(b)\n",
        "tr_bs, gpt_bs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19.601438637698003, 20.97997651203749)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AArVKcNR2CIe"
      },
      "source": [
        "Syllables number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRxKiWXp5DIW",
        "outputId": "98e76481-2c61-4123-b5ce-d7dc0324cd84"
      },
      "source": [
        "orig = sum(textstat.syllable_count(i) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.syllable_count(i) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26.924544920728128, 41.74515560775103)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN4fjafU2Ec3"
      },
      "source": [
        "Words number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1inDH3SU5DPt",
        "outputId": "073d790f-8764-42ef-8373-ad58938a95b3"
      },
      "source": [
        "orig = sum(textstat.lexicon_count(i, removepunct=True) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.lexicon_count(i, removepunct=True) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10.268937169700529, 16.581620669406927)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeLr6cW2Y64"
      },
      "source": [
        "### GPT baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhKjQDt73B3x"
      },
      "source": [
        "Flesh Kincaid Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBmXtAO26wG"
      },
      "source": [
        "a, b = test_data['gpt_bs'].values, test_data['gpt_bs'].values\n",
        "tr_bs = sum(textstat.flesch_kincaid_grade(i) for i in a)/len(a)\n",
        "gpt_bs = sum(textstat.flesch_kincaid_grade(i) for i in b)/len(b)\n",
        "tr_bs, gpt_bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYlGWVz43F4g"
      },
      "source": [
        "Syllables number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B0lt5bv29SQ"
      },
      "source": [
        "orig = sum(textstat.syllable_count(i) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.syllable_count(i) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reUqQCbh3Hi3"
      },
      "source": [
        "Words number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14nZLc6729au"
      },
      "source": [
        "orig = sum(textstat.lexicon_count(i, removepunct=True) for i in a)/len(a)\n",
        "simpl_ref = sum(textstat.lexicon_count(i, removepunct=True) for i in b)/len(b)\n",
        "orig, simpl_ref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWxKRmyZ0n1r"
      },
      "source": [
        "# ???????\n",
        "# >>> textstat.flesch_reading_ease(test_data)\n",
        "# >>> textstat.smog_index(test_data)\n",
        "# >>> textstat.flesch_kincaid_grade(test_data)\n",
        "# >>> textstat.coleman_liau_index(test_data)\n",
        "# >>> textstat.automated_readability_index(test_data)\n",
        "# >>> textstat.dale_chall_readability_score(test_data)\n",
        "# >>> textstat.difficult_words(test_data)\n",
        "# >>> textstat.linsear_write_formula(test_data)\n",
        "# >>> textstat.gunning_fog(test_data)\n",
        "# >>> textstat.text_standard(test_data)\n",
        "# >>> textstat.fernandez_huerta(test_data)\n",
        "# >>> textstat.szigriszt_pazos(test_data)\n",
        "# >>> textstat.gutierrez_polini(test_data)\n",
        "# >>> textstat.crawford(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPiQHYY96KQN"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uia_nBKWLIzS"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1IVz3XC8Rm7hQCyx3xCcABjhaENrKABvF\n",
        "test_data = pd.read_csv('/content/test_baselines.csv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hNpBXc6MNr",
        "outputId": "7bb44a18-7d10-4191-9109-84f81e02375d"
      },
      "source": [
        "tool = language_tool_python.LanguageTool('ru')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading LanguageTool: 100%|██████████| 190M/190M [00:09<00:00, 19.5MB/s]\n",
            "Unzipping /tmp/tmpds0_hg1n.zip to /root/.cache/language_tool_python.\n",
            "Downloaded https://www.languagetool.org/download/LanguageTool-5.2.zip to /root/.cache/language_tool_python.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kEtLL-iLlfC"
      },
      "source": [
        "def get_mistakes_summary(df_test, x, y):\n",
        "    src_test = list(df_test[x].values)\n",
        "    dst_test =list(df_test[y].values)\n",
        "    matches_src = []\n",
        "    for i in src_test:\n",
        "      matches_src.extend(tool.check(i))\n",
        "    matches_src\n",
        "\n",
        "    matches_dst = []\n",
        "    for i in dst_test:\n",
        "      matches_dst.extend(tool.check(i))\n",
        "    matches_dst\n",
        "\n",
        "    categories = set([i.category for i in matches_src+matches_dst])\n",
        "\n",
        "    categories_src = {i:0 for i in categories}\n",
        "    categories_dst = {i:0 for i in categories}\n",
        "\n",
        "    for i in matches_src:\n",
        "      categories_src[i.category]+=1\n",
        "\n",
        "    for i in matches_dst:\n",
        "      categories_dst[i.category]+=1\n",
        "      \n",
        "    return categories_src, categories_dst"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUVsDMd6Lxwq",
        "outputId": "a9046b60-5c8a-41d6-8a40-6341b1c71a0b"
      },
      "source": [
        "test_data.columns"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'INPUT:source', 'OUTPUT:output', 'trunctuation_bs', 'gpt_bs', 'cos_sim_ref', 'cos_sim_tr_bs', 'cos_sim_gpt_bs'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niFo83jd8pB3"
      },
      "source": [
        "src_errors, dst_errors = get_mistakes_summary(test_data, 'INPUT:source', 'OUTPUT:output')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvyXPIy1LtLg",
        "outputId": "aa07d059-07db-40de-8c9c-b9aee153c9e3"
      },
      "source": [
        "src_errors"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CASING': 10,\n",
              " 'EXTEND': 0,\n",
              " 'GRAMMAR': 22,\n",
              " 'LOGIC': 27,\n",
              " 'MISC': 0,\n",
              " 'PUNCTUATION': 34,\n",
              " 'STYLE': 14,\n",
              " 'TYPOGRAPHY': 33,\n",
              " 'TYPOS': 809}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmD0Ro41LtT3",
        "outputId": "ab228858-3a44-48ee-a149-9e1c90ebc3d9"
      },
      "source": [
        "dst_errors"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CASING': 144,\n",
              " 'EXTEND': 1,\n",
              " 'GRAMMAR': 110,\n",
              " 'LOGIC': 15,\n",
              " 'MISC': 28,\n",
              " 'PUNCTUATION': 106,\n",
              " 'STYLE': 7,\n",
              " 'TYPOGRAPHY': 733,\n",
              " 'TYPOS': 746}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aIXwmquL8UJ"
      },
      "source": [
        "src_errors, dst_errors = get_mistakes_summary(test_data, 'trunctuation_bs', 'gpt_bs')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR6kXDioMD0m",
        "outputId": "d048c50b-ee42-46d7-b89d-b46197fa92db"
      },
      "source": [
        "src_errors"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CASING': 0,\n",
              " 'GRAMMAR': 50,\n",
              " 'LOGIC': 22,\n",
              " 'MISC': 0,\n",
              " 'PUNCTUATION': 43,\n",
              " 'STYLE': 203,\n",
              " 'TYPOGRAPHY': 487,\n",
              " 'TYPOS': 441}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv7R2PgeMFag",
        "outputId": "0963e245-1e63-4dca-eac5-dd583c168663"
      },
      "source": [
        "dst_errors"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CASING': 2,\n",
              " 'GRAMMAR': 46,\n",
              " 'LOGIC': 23,\n",
              " 'MISC': 3,\n",
              " 'PUNCTUATION': 54,\n",
              " 'STYLE': 4,\n",
              " 'TYPOGRAPHY': 76,\n",
              " 'TYPOS': 697}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}