{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simplification_model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdLEB9tl608G"
      },
      "source": [
        "# Models Finetuning\n",
        "\n",
        "In this notebook several models will be finetuned to perform sentence simplification in Russian. All the models will be tuned with the parametres offered at RuSimpleSentEval competition. The main objective is not to achieve the best performance but rather compare different models trained with and without translated data. In every case training will last 5 epochs. Overall, there are five models:\n",
        "\n",
        "* Model trained on origibal WikiLarge data to perform task for English\n",
        "* Model trained on pairs: original english - simplified russian sentence. So, it learns both translate and simplify at the same time.\n",
        "* Model trained only on the translated to Russian data.\n",
        "* Model trained firstly on the original data and then on the translated corpus\n",
        "\n",
        "All the models will be evaluated and compared \n",
        "\n",
        "The first trial of training was quite unsuccessful. So, it was decided to change the approach 1) filter the data 2) use russian corpus Paraphraser\n",
        "\n",
        "The following models were trained:\n",
        "* Model trained on filtered translated data\n",
        "* Model trained on filtered english data + filtered translated data\n",
        "* Model trained on Paraphraser\n",
        "* Model trained on Paraphraser + filtered translated data\n",
        "\n",
        "P.S: this notebook is heavily based on competition https://github.com/dialogue-evaluation/RuSimpleSentEval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsBwXlp4rSVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0070c28-b3e8-4e39-e5c5-e6149ab9da20"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i32ckGda9oaO"
      },
      "source": [
        "### Necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ZzPlC152qt"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# import nltk\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIi2XjuYcMMz"
      },
      "source": [
        "! wget https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\n",
        "! tar -xzvf /content/mbart.cc25.v2.tar.gz\n",
        "! apt-get install cmake build-essential pkg-config libgoogle-perftools-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j87XOhecayN"
      },
      "source": [
        "!git clone https://github.com/google/sentencepiece.git \n",
        "%cd sentencepiece\n",
        "!mkdir build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TfHtcvLgOo3"
      },
      "source": [
        "%cd build\n",
        "!cmake ..\n",
        "!make\n",
        "!make install\n",
        "!ldconfig -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7drZkrlhKRI"
      },
      "source": [
        "# from sentencepiece git\n",
        "# !git clone https://github.com/google/sentencepiece.git \n",
        "# %cd sentencepiece\n",
        "# %mkdir build\n",
        "# %cd build\n",
        "# !cmake ..\n",
        "# !make -j $(nproc)\n",
        "# !sudo make install\n",
        "# !sudo ldconfig -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttHTJJDlNQeN",
        "outputId": "f256e636-8f73-422f-87e9-f8162a983d8f"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y97dbn4glP8"
      },
      "source": [
        "# !git clone https://github.com/pytorch/fairseq\n",
        "# !cd fairseq\n",
        "# %pip install --editable ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-bGJgZxEjs3"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHDYjuNw-W6y"
      },
      "source": [
        "### Loading data..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYRmUm-2K3Jw"
      },
      "source": [
        "I will use original WikiLarge, Google translation and Paraphraser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75RYDUVsy95"
      },
      "source": [
        "! mkdir data\n",
        "! gdown https://drive.google.com/uc?id=1bJo8TagTGKa0uyppQRqsHrKHyYO5tcZc\n",
        "! gdown https://drive.google.com/uc?id=11lqipq6ggrgCk8bVxQ4-uuPVMCKN5ebU\n",
        "! gdown https://drive.google.com/uc?id=1dB3X-Wx8qU_5nDG_pxAmLvo5H_sgnHrE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX4irYkJGL5V"
      },
      "source": [
        "% cd /content/fairseq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuPMtHlewRly"
      },
      "source": [
        "data_train = pd.read_csv('/content/wiki_train_cleaned_translated_sd.csv')\n",
        "data_dev = pd.read_csv('/content/wiki_dev_cleaned_translated_sd.csv')\n",
        "data_test  = pd.read_csv('/content/wiki_test_cleaned_translated_sd.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pvmB6gYLNsg"
      },
      "source": [
        "As a test set I use the dev part of a russian dataset collected for RuSimpleSentEval competition: https://github.com/dialogue-evaluation/RuSimpleSentEval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "331tC8esFacR"
      },
      "source": [
        "######\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/MT_sentence_simpl/wiki_test_dev_eng.csv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oo6zb0TLjP0"
      },
      "source": [
        "I get rid of the sentences where the simplified versions coincide with the original sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAVUpiAPNTBm"
      },
      "source": [
        "data_train = data_train[data_train.target_x!=data_train.target_y]\n",
        "data_dev = data_dev[data_dev.target_x!=data_dev.target_y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1jK-CaCnl6"
      },
      "source": [
        "Also, I filter data based on the simplification lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ra-U7hsCp66"
      },
      "source": [
        "dat_train = data_train[(data_train['target_y'].apply(lambda x: len(x.split(' ')))/data_train['target_x'].apply(lambda x: len(x.split(' '))))<0.8]\n",
        "\n",
        "data_train = dat_train[:82000]\n",
        "data_dev = dat_train[82000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Q2T2gausux"
      },
      "source": [
        "For additional model pretraining I use Paraphraser corpus that has proven to be quite effective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4IxN7i6YD-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2154b93a-c7ac-4d29-d4be-44de7909bf20"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1JaNqhyZf-3Fybs3iTo90__4eEN4CmhMl\n",
        "import json\n",
        "from sklearn.utils import shuffle\n",
        "with open('/content/ParaPhraserPlus.json', 'r') as f:\n",
        "  data = json.loads(f.read())\n",
        "\n",
        "import random\n",
        "src, dst = [], []\n",
        "for i in data.keys():\n",
        "  src.append(data[i]['headlines'][0])\n",
        "  dst.append(data[i]['headlines'][1])\n",
        "data = pd.DataFrame(list(zip(src, dst)), columns=['src','dst'])\n",
        "# random.shuffle(data)\n",
        "data.head(3)\n",
        "data = shuffle(data)\n",
        "data.drop_duplicates(subset=['dst'], inplace=True)\n",
        "data_new = data.sample(241000)\n",
        "data_train = data_new[:240000]\n",
        "data_dev = data_new[240000:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JaNqhyZf-3Fybs3iTo90__4eEN4CmhMl\n",
            "To: /content/ParaPhraserPlus.json\n",
            "1.11GB [00:09, 122MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbJCkcM46uWP"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dc0watrBos"
      },
      "source": [
        "! mkdir /content/fairseq/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYep--fFwsu7"
      },
      "source": [
        "### process WikiLarge\n",
        "\n",
        "with open('/content/fairseq/data/test.en', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.en', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.en', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/test.ru', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['target_y']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.ru', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['target_y']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.ru', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['target_y']+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqEIc6NkI2lr"
      },
      "source": [
        "### process WikiLarge but with Russian test\n",
        "with open('/content/fairseq/data/test.src', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['INPUT:source']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.src', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.src', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/test.dst', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['OUTPUT:output']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.dst', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['dst']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.dst', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['dst']+'\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOzlK2NKUp7",
        "outputId": "a36b9ed7-0fc0-430a-ffe7-460456f1bcc7"
      },
      "source": [
        "! echo $DATA_DIR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B73jkfOwuq5w"
      },
      "source": [
        "SPM=\"/content/sentencepiece/build/src/spm_encode\"\n",
        "BPE_MODEL=\"/content/mbart.cc25.v2/sentence.bpe.model\"\n",
        "DATA_DIR=\"/content/fairseq/data\"\n",
        "SRC=\"en\"\n",
        "TGT=\"ru\" #en\n",
        "\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/train.$SRC > $DATA_DIR/train.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/train.$TGT > $DATA_DIR/train.spm.$TGT &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/dev.$SRC > $DATA_DIR/dev.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/dev.$TGT > $DATA_DIR/dev.spm.$TGT &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/test.$SRC > $DATA_DIR/test.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/test.$TGT > $DATA_DIR/test.spm.$TGT &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohGRcSLh-lEi"
      },
      "source": [
        "\n",
        "PREPROCESSED_DATA_DIR=\"/content/fairseq/data\"\n",
        "DICT=\"/content/mbart.cc25.v2/dict.txt\"\n",
        "!fairseq-preprocess \\\n",
        "  --source-lang en \\\n",
        "  --target-lang ru \\\n",
        "  --trainpref /content/fairseq/data/train.spm \\\n",
        "  --validpref /content/fairseq/data/dev.spm \\\n",
        "  --testpref /content/fairseq/data/test.spm \\\n",
        "  --destdir /content/fairseq/data \\\n",
        "  --thresholdtgt 0 \\\n",
        "  --thresholdsrc 0 \\\n",
        "  --srcdict /content/mbart.cc25.v2/dict.txt \\\n",
        "  --tgtdict /content/mbart.cc25.v2/dict.txt \\\n",
        "  --workers 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFmmrMKh6h8W"
      },
      "source": [
        "Second training with ru-ru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qb_lVxxrFTo"
      },
      "source": [
        "! rm -r /content/fairseq/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kbxXrLyq9uQ"
      },
      "source": [
        "! mkdir /content/fairseq/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZhRLrquGYL"
      },
      "source": [
        "### process translated WikiLarge\n",
        "with open('/content/fairseq/data/test.src', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['target_x']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.src', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['target_x']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.src', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['target_x']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/test.dst', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['target_y']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.dst', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['target_y']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.dst', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['target_y']+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQNDcdvIFy2y"
      },
      "source": [
        "#### process translated WikiLarge + russian dev set as test\n",
        "with open('/content/fairseq/data/test.src', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['INPUT:source']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.src', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['target_x']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.src', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['target_x']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/test.dst', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['OUTPUT:output']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.dst', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['target_y']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.dst', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['target_y']+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwvsggN3v-iG"
      },
      "source": [
        "#### process paraphraser\n",
        "with open('/content/fairseq/data/test.src', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['INPUT:source']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.src', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.src', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['src']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/test.dst', \"a\") as f:\n",
        "  for i, row in data_test.iterrows():\n",
        "    f.write(row['OUTPUT:output']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/train.dst', \"a\") as f:\n",
        "  for i, row in data_train.iterrows():\n",
        "    f.write(row['dst']+'\\n')\n",
        "\n",
        "with open('/content/fairseq/data/dev.dst', \"a\") as f:\n",
        "  for i, row in data_dev.iterrows():\n",
        "    f.write(row['dst']+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySjc4EA7eeF3"
      },
      "source": [
        "SPM=\"/content/sentencepiece/build/src/spm_encode\"\n",
        "BPE_MODEL=\"/content/mbart.cc25.v2/sentence.bpe.model\"\n",
        "DATA_DIR=\"/content/fairseq/data\"\n",
        "SRC=\"src\"\n",
        "TGT=\"dst\"\n",
        "\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/train.$SRC > $DATA_DIR/train.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/train.$TGT > $DATA_DIR/train.spm.$TGT &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/dev.$SRC > $DATA_DIR/dev.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/dev.$TGT > $DATA_DIR/dev.spm.$TGT &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/test.$SRC > $DATA_DIR/test.spm.$SRC &\n",
        "!$SPM --model=$BPE_MODEL < $DATA_DIR/test.$TGT > $DATA_DIR/test.spm.$TGT &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ytIfDcp4Vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21afc352-2fda-4657-c256-0643700dcfe4"
      },
      "source": [
        "\n",
        "PREPROCESSED_DATA_DIR=\"/content/fairseq/data\"\n",
        "DICT=\"/content/mbart.cc25.v2/dict.txt\"\n",
        "!fairseq-preprocess \\\n",
        "  --source-lang src \\\n",
        "  --target-lang dst \\\n",
        "  --trainpref /content/fairseq/data/train.spm \\\n",
        "  --validpref /content/fairseq/data/dev.spm \\\n",
        "  --testpref /content/fairseq/data/test.spm \\\n",
        "  --destdir /content/fairseq/data \\\n",
        "  --thresholdtgt 0 \\\n",
        "  --thresholdsrc 0 \\\n",
        "  --srcdict /content/mbart.cc25.v2/dict.txt \\\n",
        "  --tgtdict /content/mbart.cc25.v2/dict.txt \\\n",
        "  --workers 70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 13:16:38 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/fairseq/data', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='src', srcdict='/content/mbart.cc25.v2/dict.txt', suppress_crashes=False, target_lang='dst', task='translation', tensorboard_logdir=None, testpref='/content/fairseq/data/test.spm', tgtdict='/content/mbart.cc25.v2/dict.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/fairseq/data/train.spm', use_plasma_view=False, user_dir=None, validpref='/content/fairseq/data/dev.spm', wandb_project=None, workers=70)\n",
            "2021-04-30 13:16:40 | INFO | fairseq_cli.preprocess | [src] Dictionary: 250001 types\n",
            "2021-04-30 13:17:00 | INFO | fairseq_cli.preprocess | [src] /content/fairseq/data/train.spm.src: 240000 sents, 4323587 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:00 | INFO | fairseq_cli.preprocess | [src] Dictionary: 250001 types\n",
            "2021-04-30 13:17:08 | INFO | fairseq_cli.preprocess | [src] /content/fairseq/data/dev.spm.src: 1000 sents, 18315 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:08 | INFO | fairseq_cli.preprocess | [src] Dictionary: 250001 types\n",
            "2021-04-30 13:17:17 | INFO | fairseq_cli.preprocess | [src] /content/fairseq/data/test.spm.src: 3406 sents, 118537 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:17 | INFO | fairseq_cli.preprocess | [dst] Dictionary: 250001 types\n",
            "2021-04-30 13:17:37 | INFO | fairseq_cli.preprocess | [dst] /content/fairseq/data/train.spm.dst: 240000 sents, 4309378 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:37 | INFO | fairseq_cli.preprocess | [dst] Dictionary: 250001 types\n",
            "2021-04-30 13:17:45 | INFO | fairseq_cli.preprocess | [dst] /content/fairseq/data/dev.spm.dst: 1000 sents, 18212 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:45 | INFO | fairseq_cli.preprocess | [dst] Dictionary: 250001 types\n",
            "2021-04-30 13:17:54 | INFO | fairseq_cli.preprocess | [dst] /content/fairseq/data/test.spm.dst: 3406 sents, 82186 tokens, 0.0% replaced by <unk>\n",
            "2021-04-30 13:17:54 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/fairseq/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVQUNdQ-po4"
      },
      "source": [
        "The code for training was the same all the times, just \"src\" and \"dst\" parts were changed. So, I do not repeated it six times, but rather altered this one, putting the necessary data in it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SORI3cBePJ1C"
      },
      "source": [
        "# ! rm -r /content/drive/MyDrive/checkpoints_ru_ru_added\n",
        "! mkdir /content/drive/MyDrive/checkpoints_ru_ru_par22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mp6ZsTv_fyd"
      },
      "source": [
        "Also, it is necessary to make the following change in /content/fairseq/fairseq/tasks/translation_from_pretrained_bart.py:\n",
        "\n",
        "```\n",
        "def __init__(self, args, src_dict, tgt_dict):\n",
        "        super().__init__(args, src_dict, tgt_dict)\n",
        "        self.args = args                  # add this line !!!!!\n",
        "        self.langs = args.langs.split(\",\")\n",
        "        for d in [src_dict, tgt_dict]:\n",
        "            for l in self.langs:\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRVvpjsWALRa"
      },
      "source": [
        "The next two cells should install apex for faster training, but some error occured:("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a58XNSANLm9f",
        "outputId": "52ce95a3-8ee2-47f0-987e-153932d457fb"
      },
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10POwmcuLqvQ"
      },
      "source": [
        "# !sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-CRreJAYBT"
      },
      "source": [
        "# Training-------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WNBS1mHCV6O"
      },
      "source": [
        "# those are just some variations in parameters that I tried\n",
        "# > train_log.txt &\n",
        "#  --update-freq 1\n",
        "#  --ddp-backend no_c10d\n",
        "# --max-tokens 1024\n",
        "# --batch-size 4 2\n",
        "# --max-epoch 25\n",
        "# --fp16 \\?????\n",
        "# --update-freq? increase????\n",
        "# --update-freq 2??? 5??\n",
        "# 3\n",
        "# --max-tokens 300\n",
        "#  --ddp-backend no_c10d \\\n",
        "# --fp16 \\\n",
        "# --memory-efficient-fp16 \\\n",
        "# --save-interval-updates 5000 \\\n",
        "# /content/mbart.cc25.v2/model.pt\n",
        "# --max-epoch 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PVUoYwjp6uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6cd832-f7dd-4961-ba14-0d0e738ee466"
      },
      "source": [
        "!fairseq-train /content/fairseq/data \\\n",
        "  --encoder-normalize-before --decoder-normalize-before \\\n",
        "  --arch mbart_large --layernorm-embedding \\\n",
        "  --task translation_from_pretrained_bart \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\n",
        "  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\n",
        "  --lr-scheduler polynomial_decay --lr 3e-05 --warmup-updates 2500 --total-num-update 54725  \\\n",
        "  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\n",
        "  --max-tokens 1024 --update-freq 5 \\\n",
        "  --source-lang src --target-lang dst \\\n",
        "  --batch-size 16 \\\n",
        "  --memory-efficient-fp16 \\\n",
        "  --validate-interval 1 \\\n",
        "  --patience 3 \\\n",
        "  --max-epoch 10 \\\n",
        "  --save-interval 10 --keep-last-epochs 10 --keep-best-checkpoints 2 \\\n",
        "  --ddp-backend no_c10d \\\n",
        "  --seed 42 --log-format simple --log-interval 500 \\\n",
        "  --restore-file /content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt \\\n",
        "  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\n",
        "  --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN \\\n",
        "  --scoring bleu \\\n",
        "  --save-dir /content/drive/MyDrive/checkpoints_ru_ru_par22"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 13:17:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 500, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': 16, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [5], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/checkpoints_ru_ru_par22', 'restore_file': '/content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': 2, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='mbart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, azureml_logging=False, batch_size=16, batch_size_valid=16, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/fairseq/data', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, keep_best_checkpoints=2, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, label_smoothing=0.2, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=500, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=1024, max_tokens_valid=1024, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt', save_dir='/content/drive/MyDrive/checkpoints_ru_ru_par22', save_interval=10, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='src', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='dst', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='54725', tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[5], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=2500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='translation_from_pretrained_bart', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, azureml_logging=False, batch_size=16, batch_size_valid=16, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/fairseq/data', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, keep_best_checkpoints=2, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, label_smoothing=0.2, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=500, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=1024, max_tokens_valid=1024, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt', save_dir='/content/drive/MyDrive/checkpoints_ru_ru_par22', save_interval=10, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='src', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='dst', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='54725', tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[5], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=2500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 2500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 54725.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'simul_type': None}\n",
            "2021-04-30 13:18:00 | INFO | fairseq.tasks.translation | [src] dictionary: 250001 types\n",
            "2021-04-30 13:18:00 | INFO | fairseq.tasks.translation | [dst] dictionary: 250001 types\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | BARTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=250027, bias=False)\n",
            "  )\n",
            "  (classification_heads): ModuleDict()\n",
            ")\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | task: TranslationFromPretrainedBARTTask\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | model: BARTModel\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | num. shared model params: 610,851,840 (num. trained: 610,851,840)\n",
            "2021-04-30 13:18:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2021-04-30 13:18:20 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/fairseq/data/valid.src-dst.src\n",
            "2021-04-30 13:18:20 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/fairseq/data/valid.src-dst.dst\n",
            "2021-04-30 13:18:20 | INFO | fairseq.tasks.translation | /content/fairseq/data valid src-dst 1000 examples\n",
            "2021-04-30 13:18:27 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-04-30 13:18:27 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-04-30 13:18:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-04-30 13:18:27 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-04-30 13:18:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-04-30 13:18:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-04-30 13:18:27 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = 16\n",
            "2021-04-30 13:18:27 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt\n",
            "2021-04-30 13:20:52 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "2021-04-30 13:20:52 | INFO | fairseq.trainer | Loaded checkpoint /content/drive/MyDrive/checkpoints_ru_ru2/checkpoint_best.pt (epoch 6 @ 0 updates)\n",
            "2021-04-30 13:21:02 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-04-30 13:21:03 | INFO | fairseq.data.data_utils | loaded 240,000 examples from: /content/fairseq/data/train.src-dst.src\n",
            "2021-04-30 13:21:03 | INFO | fairseq.data.data_utils | loaded 240,000 examples from: /content/fairseq/data/train.src-dst.dst\n",
            "2021-04-30 13:21:03 | INFO | fairseq.tasks.translation | /content/fairseq/data train src-dst 240000 examples\n",
            "2021-04-30 13:21:03 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2021-04-30 13:21:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:364: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "2021-04-30 13:21:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "2021-04-30 13:21:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "2021-04-30 13:21:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "2021-04-30 13:21:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "2021-04-30 13:31:55 | INFO | train_inner | epoch 001:    504 / 3005 loss=8.111, nll_loss=4.828, ppl=28.4, wps=1179.4, ups=0.78, wpb=1517.4, bsz=79.9, num_updates=500, lr=6e-06, gnorm=3.949, loss_scale=8, train_wall=646, gb_free=6.2, wall=807\n",
            "2021-04-30 13:42:38 | INFO | train_inner | epoch 001:   1004 / 3005 loss=7.7, nll_loss=4.28, ppl=19.43, wps=1188.3, ups=0.78, wpb=1529.4, bsz=79.9, num_updates=1000, lr=1.2e-05, gnorm=2.296, loss_scale=8, train_wall=642, gb_free=6.2, wall=1451\n",
            "2021-04-30 13:53:17 | INFO | train_inner | epoch 001:   1504 / 3005 loss=7.501, nll_loss=4.035, ppl=16.39, wps=1180.7, ups=0.78, wpb=1508.5, bsz=79.9, num_updates=1500, lr=1.8e-05, gnorm=2.263, loss_scale=8, train_wall=637, gb_free=6.2, wall=2090\n",
            "2021-04-30 14:04:02 | INFO | train_inner | epoch 001:   2004 / 3005 loss=7.387, nll_loss=3.892, ppl=14.84, wps=1186.8, ups=0.77, wpb=1531.4, bsz=79.9, num_updates=2000, lr=2.4e-05, gnorm=2.199, loss_scale=8, train_wall=643, gb_free=6.2, wall=2735\n",
            "2021-04-30 14:14:36 | INFO | train_inner | epoch 001:   2504 / 3005 loss=7.292, nll_loss=3.772, ppl=13.66, wps=1179.6, ups=0.79, wpb=1496, bsz=79.9, num_updates=2500, lr=3e-05, gnorm=2.165, loss_scale=8, train_wall=632, gb_free=6.2, wall=3369\n",
            "2021-04-30 14:25:15 | INFO | train_inner | epoch 001:   3004 / 3005 loss=7.184, nll_loss=3.638, ppl=12.45, wps=1177.7, ups=0.78, wpb=1503.8, bsz=79.9, num_updates=3000, lr=2.97128e-05, gnorm=2.101, loss_scale=8, train_wall=637, gb_free=6.2, wall=4007\n",
            "2021-04-30 14:25:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 14:25:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.797 | nll_loss 2.958 | ppl 7.77 | wps 3351.3 | wpb 305 | bsz 15.9 | num_updates 3001\n",
            "2021-04-30 14:25:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-04-30 14:25:21 | INFO | train | epoch 001 | loss 7.53 | nll_loss 4.075 | ppl 16.86 | wps 1180.3 | ups 0.78 | wpb 1514 | bsz 79.9 | num_updates 3001 | lr 2.97122e-05 | gnorm 2.496 | loss_scale 8 | train_wall 3837 | gb_free 6.2 | wall 4013\n",
            "2021-04-30 14:25:21 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2021-04-30 14:25:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 14:35:59 | INFO | train_inner | epoch 002:    499 / 3005 loss=7.112, nll_loss=3.55, ppl=11.71, wps=1168.5, ups=0.78, wpb=1505.3, bsz=79.7, num_updates=3500, lr=2.94256e-05, gnorm=2.061, loss_scale=16, train_wall=636, gb_free=6.2, wall=4651\n",
            "2021-04-30 14:46:38 | INFO | train_inner | epoch 002:    999 / 3005 loss=7.061, nll_loss=3.49, ppl=11.23, wps=1183.1, ups=0.78, wpb=1513, bsz=79.9, num_updates=4000, lr=2.91383e-05, gnorm=2.025, loss_scale=16, train_wall=637, gb_free=6.2, wall=5291\n",
            "2021-04-30 14:57:18 | INFO | train_inner | epoch 002:   1499 / 3005 loss=7.032, nll_loss=3.458, ppl=10.99, wps=1180.1, ups=0.78, wpb=1511.3, bsz=79.9, num_updates=4500, lr=2.88511e-05, gnorm=1.974, loss_scale=16, train_wall=638, gb_free=6.2, wall=5931\n",
            "2021-04-30 15:03:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "2021-04-30 15:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "2021-04-30 15:08:01 | INFO | train_inner | epoch 002:   2001 / 3005 loss=7.011, nll_loss=3.431, ppl=10.79, wps=1178.1, ups=0.78, wpb=1515.1, bsz=79.8, num_updates=5000, lr=2.85639e-05, gnorm=2.073, loss_scale=4, train_wall=641, gb_free=6.2, wall=6574\n",
            "2021-04-30 15:15:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "2021-04-30 15:16:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
            "2021-04-30 15:18:46 | INFO | train_inner | epoch 002:   2503 / 3005 loss=6.97, nll_loss=3.384, ppl=10.44, wps=1174.8, ups=0.78, wpb=1514.6, bsz=79.9, num_updates=5500, lr=2.82767e-05, gnorm=2.537, loss_scale=1, train_wall=643, gb_free=6.2, wall=7219\n",
            "2021-04-30 15:29:28 | INFO | train_inner | epoch 002:   3003 / 3005 loss=6.96, nll_loss=3.377, ppl=10.39, wps=1188.3, ups=0.78, wpb=1524.5, bsz=79.9, num_updates=6000, lr=2.79895e-05, gnorm=2.66, loss_scale=1, train_wall=639, gb_free=6.2, wall=7860\n",
            "2021-04-30 15:29:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 15:29:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.667 | nll_loss 2.815 | ppl 7.04 | wps 3341.9 | wpb 305 | bsz 15.9 | num_updates 6002\n",
            "2021-04-30 15:29:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-04-30 15:29:35 | INFO | train | epoch 002 | loss 7.024 | nll_loss 3.448 | ppl 10.91 | wps 1178.8 | ups 0.78 | wpb 1514 | bsz 79.9 | num_updates 6002 | lr 2.79883e-05 | gnorm 2.222 | loss_scale 1 | train_wall 3837 | gb_free 6.2 | wall 7868\n",
            "2021-04-30 15:29:35 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2021-04-30 15:29:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 15:29:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
            "2021-04-30 15:40:12 | INFO | train_inner | epoch 003:    499 / 3005 loss=6.866, nll_loss=3.259, ppl=9.57, wps=1163.4, ups=0.78, wpb=1500.2, bsz=79.7, num_updates=6500, lr=2.77022e-05, gnorm=2.797, loss_scale=0.5, train_wall=637, gb_free=6.2, wall=8505\n",
            "2021-04-30 15:50:52 | INFO | train_inner | epoch 003:    999 / 3005 loss=6.871, nll_loss=3.269, ppl=9.64, wps=1181.2, ups=0.78, wpb=1512, bsz=79.9, num_updates=7000, lr=2.7415e-05, gnorm=1.853, loss_scale=0.5, train_wall=638, gb_free=6.2, wall=9145\n",
            "2021-04-30 16:01:36 | INFO | train_inner | epoch 003:   1499 / 3005 loss=6.849, nll_loss=3.243, ppl=9.47, wps=1186.1, ups=0.78, wpb=1526.5, bsz=79.9, num_updates=7500, lr=2.71278e-05, gnorm=1.838, loss_scale=0.5, train_wall=642, gb_free=6.2, wall=9789\n",
            "2021-04-30 16:12:19 | INFO | train_inner | epoch 003:   1999 / 3005 loss=6.852, nll_loss=3.249, ppl=9.51, wps=1188.8, ups=0.78, wpb=1529.6, bsz=79.9, num_updates=8000, lr=2.68406e-05, gnorm=1.821, loss_scale=0.5, train_wall=641, gb_free=6.2, wall=10432\n",
            "2021-04-30 16:22:55 | INFO | train_inner | epoch 003:   2499 / 3005 loss=6.798, nll_loss=3.184, ppl=9.09, wps=1176.8, ups=0.79, wpb=1495.6, bsz=79.9, num_updates=8500, lr=2.65534e-05, gnorm=1.829, loss_scale=0.5, train_wall=634, gb_free=6.2, wall=11067\n",
            "2021-04-30 16:33:37 | INFO | train_inner | epoch 003:   2999 / 3005 loss=6.807, nll_loss=3.198, ppl=9.18, wps=1183.6, ups=0.78, wpb=1520.2, bsz=79.9, num_updates=9000, lr=2.62662e-05, gnorm=1.859, loss_scale=0.5, train_wall=640, gb_free=6.2, wall=11710\n",
            "2021-04-30 16:33:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 16:33:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.579 | nll_loss 2.699 | ppl 6.49 | wps 3336.9 | wpb 305 | bsz 15.9 | num_updates 9006\n",
            "2021-04-30 16:33:50 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-04-30 16:33:50 | INFO | train | epoch 003 | loss 6.841 | nll_loss 3.234 | ppl 9.41 | wps 1180 | ups 0.78 | wpb 1514 | bsz 79.9 | num_updates 9006 | lr 2.62627e-05 | gnorm 1.999 | loss_scale 0.5 | train_wall 3837 | gb_free 6.2 | wall 11722\n",
            "2021-04-30 16:33:50 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2021-04-30 16:33:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 16:44:22 | INFO | train_inner | epoch 004:    494 / 3005 loss=6.765, nll_loss=3.143, ppl=8.83, wps=1170.2, ups=0.78, wpb=1508.7, bsz=79.8, num_updates=9500, lr=2.59789e-05, gnorm=1.781, loss_scale=1, train_wall=637, gb_free=6.2, wall=12354\n",
            "2021-04-30 16:55:00 | INFO | train_inner | epoch 004:    994 / 3005 loss=6.753, nll_loss=3.13, ppl=8.76, wps=1181.8, ups=0.78, wpb=1509.3, bsz=79.8, num_updates=10000, lr=2.56917e-05, gnorm=1.786, loss_scale=1, train_wall=637, gb_free=6.2, wall=12993\n",
            "2021-04-30 17:05:43 | INFO | train_inner | epoch 004:   1494 / 3005 loss=6.755, nll_loss=3.134, ppl=8.78, wps=1186.7, ups=0.78, wpb=1525.5, bsz=79.9, num_updates=10500, lr=2.54045e-05, gnorm=1.754, loss_scale=1, train_wall=641, gb_free=6.2, wall=13635\n",
            "2021-04-30 17:16:21 | INFO | train_inner | epoch 004:   1994 / 3005 loss=6.724, nll_loss=3.098, ppl=8.56, wps=1178.2, ups=0.78, wpb=1504.2, bsz=79.9, num_updates=11000, lr=2.51173e-05, gnorm=1.756, loss_scale=1, train_wall=636, gb_free=6.2, wall=14274\n",
            "2021-04-30 17:27:04 | INFO | train_inner | epoch 004:   2494 / 3005 loss=6.725, nll_loss=3.101, ppl=8.58, wps=1185.7, ups=0.78, wpb=1523.3, bsz=79.9, num_updates=11500, lr=2.48301e-05, gnorm=1.754, loss_scale=1, train_wall=640, gb_free=6.2, wall=14916\n",
            "2021-04-30 17:37:45 | INFO | train_inner | epoch 004:   2994 / 3005 loss=6.71, nll_loss=3.083, ppl=8.47, wps=1180.4, ups=0.78, wpb=1513.9, bsz=79.9, num_updates=12000, lr=2.45428e-05, gnorm=1.769, loss_scale=1, train_wall=639, gb_free=6.2, wall=15557\n",
            "2021-04-30 17:37:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 17:38:03 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.542 | nll_loss 2.655 | ppl 6.3 | wps 3334.9 | wpb 305 | bsz 15.9 | num_updates 12011\n",
            "2021-04-30 17:38:03 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-04-30 17:38:03 | INFO | train | epoch 004 | loss 6.738 | nll_loss 3.114 | ppl 8.66 | wps 1180.5 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 12011 | lr 2.45365e-05 | gnorm 1.77 | loss_scale 1 | train_wall 3836 | gb_free 6.2 | wall 15576\n",
            "2021-04-30 17:38:03 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2021-04-30 17:38:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 17:48:28 | INFO | train_inner | epoch 005:    489 / 3005 loss=6.693, nll_loss=3.061, ppl=8.34, wps=1170.3, ups=0.78, wpb=1506.1, bsz=79.7, num_updates=12500, lr=2.42556e-05, gnorm=1.794, loss_scale=1, train_wall=635, gb_free=6.2, wall=16201\n",
            "2021-04-30 17:59:08 | INFO | train_inner | epoch 005:    989 / 3005 loss=6.672, nll_loss=3.035, ppl=8.2, wps=1182.8, ups=0.78, wpb=1514.1, bsz=79.9, num_updates=13000, lr=2.39684e-05, gnorm=1.733, loss_scale=2, train_wall=638, gb_free=5.6, wall=16841\n",
            "2021-04-30 18:09:47 | INFO | train_inner | epoch 005:   1489 / 3005 loss=6.648, nll_loss=3.006, ppl=8.04, wps=1178.1, ups=0.78, wpb=1504.5, bsz=79.9, num_updates=13500, lr=2.36812e-05, gnorm=1.719, loss_scale=2, train_wall=637, gb_free=6.2, wall=17479\n",
            "2021-04-30 18:20:27 | INFO | train_inner | epoch 005:   1989 / 3005 loss=6.681, nll_loss=3.05, ppl=8.28, wps=1179.7, ups=0.78, wpb=1509.6, bsz=79.9, num_updates=14000, lr=2.3394e-05, gnorm=1.736, loss_scale=2, train_wall=638, gb_free=6.2, wall=18119\n",
            "2021-04-30 18:31:10 | INFO | train_inner | epoch 005:   2489 / 3005 loss=6.664, nll_loss=3.028, ppl=8.16, wps=1186.4, ups=0.78, wpb=1525.6, bsz=79.9, num_updates=14500, lr=2.31067e-05, gnorm=1.735, loss_scale=2, train_wall=641, gb_free=6.2, wall=18762\n",
            "2021-04-30 18:41:53 | INFO | train_inner | epoch 005:   2989 / 3005 loss=6.644, nll_loss=3.006, ppl=8.03, wps=1182.7, ups=0.78, wpb=1520.7, bsz=79.9, num_updates=15000, lr=2.28195e-05, gnorm=1.692, loss_scale=2, train_wall=641, gb_free=6.2, wall=19405\n",
            "2021-04-30 18:42:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 18:42:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.485 | nll_loss 2.618 | ppl 6.14 | wps 3350.2 | wpb 305 | bsz 15.9 | num_updates 15016\n",
            "2021-04-30 18:42:18 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-04-30 18:42:18 | INFO | train | epoch 005 | loss 6.668 | nll_loss 3.032 | ppl 8.18 | wps 1180.2 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 15016 | lr 2.28103e-05 | gnorm 1.731 | loss_scale 2 | train_wall 3837 | gb_free 6.2 | wall 19431\n",
            "2021-04-30 18:42:18 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2021-04-30 18:42:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 18:52:43 | INFO | train_inner | epoch 006:    484 / 3005 loss=6.626, nll_loss=2.98, ppl=7.89, wps=1174.3, ups=0.77, wpb=1526.7, bsz=79.8, num_updates=15500, lr=2.25323e-05, gnorm=1.708, loss_scale=2, train_wall=642, gb_free=6.2, wall=20055\n",
            "2021-04-30 19:03:24 | INFO | train_inner | epoch 006:    984 / 3005 loss=6.632, nll_loss=2.989, ppl=7.94, wps=1182.7, ups=0.78, wpb=1518.3, bsz=79.8, num_updates=16000, lr=2.22451e-05, gnorm=1.724, loss_scale=4, train_wall=640, gb_free=6.2, wall=20697\n",
            "2021-04-30 19:14:04 | INFO | train_inner | epoch 006:   1484 / 3005 loss=6.622, nll_loss=2.978, ppl=7.88, wps=1186.9, ups=0.78, wpb=1517.7, bsz=79.9, num_updates=16500, lr=2.19579e-05, gnorm=1.703, loss_scale=4, train_wall=637, gb_free=6.2, wall=21336\n",
            "2021-04-30 19:24:45 | INFO | train_inner | epoch 006:   1984 / 3005 loss=6.618, nll_loss=2.974, ppl=7.86, wps=1180.9, ups=0.78, wpb=1515.4, bsz=79.9, num_updates=17000, lr=2.16707e-05, gnorm=1.719, loss_scale=4, train_wall=640, gb_free=6.2, wall=21978\n",
            "2021-04-30 19:35:23 | INFO | train_inner | epoch 006:   2484 / 3005 loss=6.6, nll_loss=2.953, ppl=7.74, wps=1180.4, ups=0.78, wpb=1505.1, bsz=79.9, num_updates=17500, lr=2.13834e-05, gnorm=1.73, loss_scale=4, train_wall=636, gb_free=6.2, wall=22616\n",
            "2021-04-30 19:46:00 | INFO | train_inner | epoch 006:   2984 / 3005 loss=6.604, nll_loss=2.959, ppl=7.78, wps=1177.2, ups=0.79, wpb=1499.6, bsz=79.9, num_updates=18000, lr=2.10962e-05, gnorm=1.685, loss_scale=4, train_wall=635, gb_free=6.2, wall=23253\n",
            "2021-04-30 19:46:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 19:46:32 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.477 | nll_loss 2.583 | ppl 5.99 | wps 3352.2 | wpb 305 | bsz 15.9 | num_updates 18021\n",
            "2021-04-30 19:46:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-04-30 19:46:32 | INFO | train | epoch 006 | loss 6.616 | nll_loss 2.971 | ppl 7.84 | wps 1180.5 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 18021 | lr 2.10842e-05 | gnorm 1.711 | loss_scale 4 | train_wall 3836 | gb_free 6.2 | wall 23285\n",
            "2021-04-30 19:46:32 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2021-04-30 19:46:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 19:56:44 | INFO | train_inner | epoch 007:    479 / 3005 loss=6.572, nll_loss=2.917, ppl=7.55, wps=1171.3, ups=0.78, wpb=1508.3, bsz=79.8, num_updates=18500, lr=2.0809e-05, gnorm=1.68, loss_scale=4, train_wall=636, gb_free=6.2, wall=23896\n",
            "2021-04-30 20:07:23 | INFO | train_inner | epoch 007:    979 / 3005 loss=6.576, nll_loss=2.923, ppl=7.59, wps=1182.5, ups=0.78, wpb=1512.5, bsz=79.9, num_updates=19000, lr=2.05218e-05, gnorm=1.684, loss_scale=4, train_wall=638, gb_free=6.2, wall=24536\n",
            "2021-04-30 20:17:59 | INFO | train_inner | epoch 007:   1479 / 3005 loss=6.574, nll_loss=2.922, ppl=7.58, wps=1179.5, ups=0.79, wpb=1500.6, bsz=79.9, num_updates=19500, lr=2.02346e-05, gnorm=1.687, loss_scale=8, train_wall=634, gb_free=6.2, wall=25172\n",
            "2021-04-30 20:28:41 | INFO | train_inner | epoch 007:   1979 / 3005 loss=6.594, nll_loss=2.946, ppl=7.71, wps=1186.7, ups=0.78, wpb=1523.2, bsz=79.9, num_updates=20000, lr=1.99473e-05, gnorm=1.684, loss_scale=8, train_wall=640, gb_free=6.2, wall=25814\n",
            "2021-04-30 20:39:23 | INFO | train_inner | epoch 007:   2479 / 3005 loss=6.584, nll_loss=2.934, ppl=7.64, wps=1188.3, ups=0.78, wpb=1526.6, bsz=80, num_updates=20500, lr=1.96601e-05, gnorm=1.674, loss_scale=8, train_wall=641, gb_free=6.2, wall=26456\n",
            "2021-04-30 20:50:03 | INFO | train_inner | epoch 007:   2979 / 3005 loss=6.56, nll_loss=2.905, ppl=7.49, wps=1185.4, ups=0.78, wpb=1516.8, bsz=79.8, num_updates=21000, lr=1.93729e-05, gnorm=1.704, loss_scale=8, train_wall=638, gb_free=6.2, wall=27096\n",
            "2021-04-30 20:50:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 20:50:41 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.46 | nll_loss 2.57 | ppl 5.94 | wps 3351.4 | wpb 305 | bsz 15.9 | num_updates 21026\n",
            "2021-04-30 20:50:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-04-30 20:50:41 | INFO | train | epoch 007 | loss 6.575 | nll_loss 2.923 | ppl 7.58 | wps 1182 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 21026 | lr 1.9358e-05 | gnorm 1.686 | loss_scale 8 | train_wall 3832 | gb_free 6.2 | wall 27134\n",
            "2021-04-30 20:50:41 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2021-04-30 20:50:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 21:00:47 | INFO | train_inner | epoch 008:    474 / 3005 loss=6.537, nll_loss=2.876, ppl=7.34, wps=1173, ups=0.78, wpb=1510.5, bsz=79.8, num_updates=21500, lr=1.90857e-05, gnorm=1.671, loss_scale=8, train_wall=636, gb_free=6.2, wall=27740\n",
            "2021-04-30 21:11:22 | INFO | train_inner | epoch 008:    974 / 3005 loss=6.538, nll_loss=2.877, ppl=7.35, wps=1178.5, ups=0.79, wpb=1497.4, bsz=79.8, num_updates=22000, lr=1.87985e-05, gnorm=1.684, loss_scale=8, train_wall=634, gb_free=6.2, wall=28375\n",
            "2021-04-30 21:22:03 | INFO | train_inner | epoch 008:   1474 / 3005 loss=6.543, nll_loss=2.884, ppl=7.38, wps=1182, ups=0.78, wpb=1514.2, bsz=79.9, num_updates=22500, lr=1.85112e-05, gnorm=1.67, loss_scale=16, train_wall=639, gb_free=6.2, wall=29016\n",
            "2021-04-30 21:32:45 | INFO | train_inner | epoch 008:   1974 / 3005 loss=6.544, nll_loss=2.886, ppl=7.39, wps=1186.5, ups=0.78, wpb=1523.4, bsz=79.9, num_updates=23000, lr=1.8224e-05, gnorm=1.668, loss_scale=16, train_wall=640, gb_free=6.2, wall=29658\n",
            "2021-04-30 21:43:24 | INFO | train_inner | epoch 008:   2474 / 3005 loss=6.547, nll_loss=2.89, ppl=7.41, wps=1185, ups=0.78, wpb=1514.5, bsz=79.9, num_updates=23500, lr=1.79368e-05, gnorm=1.682, loss_scale=16, train_wall=637, gb_free=6.2, wall=30297\n",
            "2021-04-30 21:54:06 | INFO | train_inner | epoch 008:   2974 / 3005 loss=6.545, nll_loss=2.89, ppl=7.41, wps=1185.8, ups=0.78, wpb=1521.6, bsz=79.9, num_updates=24000, lr=1.76496e-05, gnorm=1.654, loss_scale=16, train_wall=640, gb_free=6.2, wall=30938\n",
            "2021-04-30 21:54:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 21:54:50 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.437 | nll_loss 2.551 | ppl 5.86 | wps 3343.4 | wpb 305 | bsz 15.9 | num_updates 24031\n",
            "2021-04-30 21:54:50 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-04-30 21:54:50 | INFO | train | epoch 008 | loss 6.543 | nll_loss 2.885 | ppl 7.39 | wps 1182 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 24031 | lr 1.76318e-05 | gnorm 1.671 | loss_scale 16 | train_wall 3832 | gb_free 6.2 | wall 30983\n",
            "2021-04-30 21:54:50 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2021-04-30 21:54:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 22:04:50 | INFO | train_inner | epoch 009:    469 / 3005 loss=6.519, nll_loss=2.854, ppl=7.23, wps=1173.4, ups=0.78, wpb=1513.4, bsz=79.7, num_updates=24500, lr=1.73624e-05, gnorm=1.663, loss_scale=16, train_wall=637, gb_free=6.2, wall=31583\n",
            "2021-04-30 22:15:31 | INFO | train_inner | epoch 009:    969 / 3005 loss=6.519, nll_loss=2.855, ppl=7.23, wps=1183.6, ups=0.78, wpb=1516.4, bsz=79.8, num_updates=25000, lr=1.70752e-05, gnorm=1.654, loss_scale=16, train_wall=639, gb_free=6.2, wall=32224\n",
            "2021-04-30 22:26:15 | INFO | train_inner | epoch 009:   1469 / 3005 loss=6.534, nll_loss=2.874, ppl=7.33, wps=1190.6, ups=0.78, wpb=1533.5, bsz=79.9, num_updates=25500, lr=1.67879e-05, gnorm=1.652, loss_scale=16, train_wall=642, gb_free=6.2, wall=32868\n",
            "2021-04-30 22:36:53 | INFO | train_inner | epoch 009:   1969 / 3005 loss=6.514, nll_loss=2.851, ppl=7.21, wps=1185.1, ups=0.78, wpb=1513.4, bsz=79.9, num_updates=26000, lr=1.65007e-05, gnorm=1.652, loss_scale=32, train_wall=637, gb_free=6.2, wall=33506\n",
            "2021-04-30 22:47:31 | INFO | train_inner | epoch 009:   2469 / 3005 loss=6.495, nll_loss=2.828, ppl=7.1, wps=1178.6, ups=0.78, wpb=1503.3, bsz=80, num_updates=26500, lr=1.62135e-05, gnorm=1.658, loss_scale=32, train_wall=636, gb_free=6.2, wall=34144\n",
            "2021-04-30 22:58:08 | INFO | train_inner | epoch 009:   2969 / 3005 loss=6.513, nll_loss=2.85, ppl=7.21, wps=1181.5, ups=0.79, wpb=1504.2, bsz=79.9, num_updates=27000, lr=1.59263e-05, gnorm=1.659, loss_scale=32, train_wall=635, gb_free=6.2, wall=34780\n",
            "2021-04-30 22:58:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-04-30 22:58:59 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.44 | nll_loss 2.553 | ppl 5.87 | wps 3354.8 | wpb 305 | bsz 15.9 | num_updates 27036\n",
            "2021-04-30 22:58:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-04-30 22:58:59 | INFO | train | epoch 009 | loss 6.516 | nll_loss 2.852 | ppl 7.22 | wps 1182 | ups 0.78 | wpb 1513.9 | bsz 79.9 | num_updates 27036 | lr 1.59056e-05 | gnorm 1.656 | loss_scale 32 | train_wall 3832 | gb_free 6.2 | wall 34831\n",
            "2021-04-30 22:58:59 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2021-04-30 22:58:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-04-30 23:08:57 | INFO | train_inner | epoch 010:    464 / 3005 loss=6.508, nll_loss=2.842, ppl=7.17, wps=1179.2, ups=0.77, wpb=1531.2, bsz=79.8, num_updates=27500, lr=1.56391e-05, gnorm=1.645, loss_scale=32, train_wall=642, gb_free=6.2, wall=35430\n",
            "2021-04-30 23:19:35 | INFO | train_inner | epoch 010:    964 / 3005 loss=6.492, nll_loss=2.824, ppl=7.08, wps=1179.7, ups=0.78, wpb=1505.4, bsz=79.8, num_updates=28000, lr=1.53518e-05, gnorm=1.651, loss_scale=32, train_wall=636, gb_free=5.5, wall=36068\n",
            "2021-04-30 23:30:12 | INFO | train_inner | epoch 010:   1464 / 3005 loss=6.492, nll_loss=2.822, ppl=7.07, wps=1177.9, ups=0.79, wpb=1500.3, bsz=79.9, num_updates=28500, lr=1.50646e-05, gnorm=1.67, loss_scale=32, train_wall=635, gb_free=6.2, wall=36705\n",
            "2021-04-30 23:40:49 | INFO | train_inner | epoch 010:   1964 / 3005 loss=6.497, nll_loss=2.83, ppl=7.11, wps=1185.1, ups=0.78, wpb=1510.3, bsz=80, num_updates=29000, lr=1.47774e-05, gnorm=1.645, loss_scale=64, train_wall=635, gb_free=5.9, wall=37342\n",
            "2021-04-30 23:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "2021-04-30 23:51:29 | INFO | train_inner | epoch 010:   2465 / 3005 loss=6.486, nll_loss=2.817, ppl=7.05, wps=1181.2, ups=0.78, wpb=1511.6, bsz=79.9, num_updates=29500, lr=1.44902e-05, gnorm=1.643, loss_scale=32, train_wall=638, gb_free=6.2, wall=37982\n",
            "2021-05-01 00:02:10 | INFO | train_inner | epoch 010:   2965 / 3005 loss=6.493, nll_loss=2.825, ppl=7.09, wps=1185.9, ups=0.78, wpb=1521.4, bsz=79.9, num_updates=30000, lr=1.4203e-05, gnorm=1.64, loss_scale=32, train_wall=640, gb_free=6.2, wall=38623\n",
            "2021-05-01 00:03:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-05-01 00:03:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.426 | nll_loss 2.538 | ppl 5.81 | wps 3365.5 | wpb 305 | bsz 15.9 | num_updates 30040\n",
            "2021-05-01 00:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 30040 updates\n",
            "2021-05-01 00:03:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/checkpoints_ru_ru_par22/checkpoint10.pt\n",
            "2021-05-01 00:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/checkpoints_ru_ru_par22/checkpoint10.pt\n",
            "2021-05-01 00:12:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/checkpoints_ru_ru_par22/checkpoint10.pt (epoch 10 @ 30040 updates, score 6.426) (writing took 564.1140458739974 seconds)\n",
            "2021-05-01 00:12:32 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-05-01 00:12:32 | INFO | train | epoch 010 | loss 6.494 | nll_loss 2.826 | ppl 7.09 | wps 1030.5 | ups 0.68 | wpb 1513.9 | bsz 79.9 | num_updates 30040 | lr 1.418e-05 | gnorm 1.649 | loss_scale 32 | train_wall 3832 | gb_free 6.2 | wall 39245\n",
            "2021-05-01 00:12:32 | INFO | fairseq_cli.train | done training in 39089.2 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGGmjSE_JLO"
      },
      "source": [
        "### Test to check that everything is ok and get prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpbpDtlCCY5j"
      },
      "source": [
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_s-v3np7l_"
      },
      "source": [
        "!fairseq-generate /content/fairseq/data \\\n",
        "  --path  /content/drive/MyDrive/checkpoints_ru_ru_par22/checkpoint_best.pt \\\n",
        "  --task translation_from_pretrained_bart \\\n",
        "  --gen-subset test \\\n",
        "  --source-lang src --target-lang dst \\\n",
        "  --bpe 'sentencepiece' --sentencepiece-model /content/mbart.cc25.v2/sentence.bpe.model \\\n",
        "  --sacrebleu --remove-bpe 'sentencepiece' \\\n",
        "  --batch-size 32 --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN > model_prediction.txt & \n",
        "!cat model_prediction.txt | grep -P \"^H\" |sort -V |cut -f 3- > model_prediction_ru_par22_filtered15.hyp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3bcXrIBE482"
      },
      "source": [
        "!cp /content/model_prediction_ru22_par_filtered15.hyp /content/drive/MyDrive/MT_sentence_simpl/predictions/model_prediction_ru22_par_filtered15.hyp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z33ymqaXA9Ci"
      },
      "source": [
        "# Also, try SARI evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imXXZKIKqXjK",
        "outputId": "007cb0a5-19a9-4257-8dae-dcda9dba06a6"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxVrDxwiqXpG"
      },
      "source": [
        "! git clone https://github.com/feralvam/easse\n",
        "! git clone https://github.com/Andoree/sent_simplification.git\n",
        "%cp /content/sent_simplification/sari.py /content/easse/easse\n",
        "%cd easse\n",
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINU32l7rajY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d48d6d-7835-457e-f522-1dbb2cfab9d4"
      },
      "source": [
        "%cd /content\n",
        "! mkdir prepared_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch6UCNu4BF3_"
      },
      "source": [
        "Prepare data for SARI calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bScRNyEuO5LG",
        "outputId": "e62d097a-ba46-4199-d776-1f6b12ea876c"
      },
      "source": [
        "! python /content/sent_simplification/refs_to_easse_format.py \\\n",
        "--input_path /content/drive/MyDrive/MT_sentence_simpl/wiki_test_dev_eng.csv \\\n",
        "--output_dataset_name test_ref_data \\\n",
        "--src_column \"INPUT:source\" \\\n",
        "--trg_column \"OUTPUT:output\" \\\n",
        "--output_dir /content/prepared_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "3406\n",
            "3406\n",
            "Overall number of references: 3406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrq--5MYOwHr"
      },
      "source": [
        "with open('/content/model_prediction_ru_ru_filtered15.hyp', 'r') as f:\n",
        "  sentences = [i.strip() for i in f.readlines()]\n",
        "\n",
        "lt = list()\n",
        "st = set()\n",
        "for i in sentences:\n",
        "  if i not in st:\n",
        "    lt.append(i)\n",
        "    st.add(i)\n",
        "\n",
        "with open('/content/model_prediction_ru_ru_filtered15.test.hyp', 'w') as f:\n",
        "  for i in lt:\n",
        "    f.write(i+'\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMaGo5MGrlaG",
        "outputId": "f5d8226f-0067-4610-afec-ec0b8b831476"
      },
      "source": [
        "! easse evaluate \\\n",
        "--test_set custom \\\n",
        "--metrics sari \\\n",
        "--refs_sents_paths /content/prepared_data/test_ref_data.ref.0,/content/prepared_data/test_ref_data.ref.1,/content/prepared_data/test_ref_data.ref.2,/content/prepared_data/test_ref_data.ref.3,/content/prepared_data/test_ref_data.ref.4 \\\n",
        "--orig_sents_path /content/prepared_data/test_ref_data.src \\\n",
        "--sys_sents_path /content/model_prediction_ru_ru_filtered15.test.hyp -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sari': 32.957, 'quality_estimation': {'Compression ratio': 0.635, 'Sentence splits': 0.987, 'Levenshtein similarity': 0.757, 'Exact copies': 0.008, 'Additions proportion': 0.019, 'Deletions proportion': 0.392, 'Lexical complexity score': 10.724}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}